[2025-06-12 10:25:26,528]: 
Training ResNet18 with parametrized_relu
[2025-06-12 10:26:52,235]: [ResNet18_parametrized_relu] Epoch: 001 Train Loss: 1.4908 Train Acc: 0.4512 Eval Loss: 1.3421 Eval Acc: 0.5215 (LR: 0.00100000)
[2025-06-12 10:28:17,346]: [ResNet18_parametrized_relu] Epoch: 002 Train Loss: 1.0604 Train Acc: 0.6215 Eval Loss: 1.0452 Eval Acc: 0.6358 (LR: 0.00100000)
[2025-06-12 10:29:42,757]: [ResNet18_parametrized_relu] Epoch: 003 Train Loss: 0.8734 Train Acc: 0.6926 Eval Loss: 0.8117 Eval Acc: 0.7218 (LR: 0.00100000)
[2025-06-12 10:31:07,799]: [ResNet18_parametrized_relu] Epoch: 004 Train Loss: 0.7440 Train Acc: 0.7403 Eval Loss: 0.7004 Eval Acc: 0.7590 (LR: 0.00100000)
[2025-06-12 10:32:33,116]: [ResNet18_parametrized_relu] Epoch: 005 Train Loss: 0.6558 Train Acc: 0.7725 Eval Loss: 0.7387 Eval Acc: 0.7640 (LR: 0.00100000)
[2025-06-12 10:33:58,335]: [ResNet18_parametrized_relu] Epoch: 006 Train Loss: 0.5937 Train Acc: 0.7943 Eval Loss: 0.7626 Eval Acc: 0.7586 (LR: 0.00100000)
[2025-06-12 10:35:23,535]: [ResNet18_parametrized_relu] Epoch: 007 Train Loss: 0.5491 Train Acc: 0.8094 Eval Loss: 0.6385 Eval Acc: 0.7843 (LR: 0.00100000)
[2025-06-12 10:36:48,543]: [ResNet18_parametrized_relu] Epoch: 008 Train Loss: 0.5142 Train Acc: 0.8229 Eval Loss: 0.6406 Eval Acc: 0.7938 (LR: 0.00100000)
[2025-06-12 10:38:13,567]: [ResNet18_parametrized_relu] Epoch: 009 Train Loss: 0.4773 Train Acc: 0.8353 Eval Loss: 0.5404 Eval Acc: 0.8206 (LR: 0.00100000)
[2025-06-12 10:39:38,601]: [ResNet18_parametrized_relu] Epoch: 010 Train Loss: 0.4507 Train Acc: 0.8453 Eval Loss: 0.5486 Eval Acc: 0.8268 (LR: 0.00100000)
[2025-06-12 10:41:17,931]: [ResNet18_parametrized_relu] Epoch: 011 Train Loss: 0.4294 Train Acc: 0.8518 Eval Loss: 0.6197 Eval Acc: 0.8012 (LR: 0.00100000)
[2025-06-12 10:43:04,166]: [ResNet18_parametrized_relu] Epoch: 012 Train Loss: 0.4078 Train Acc: 0.8576 Eval Loss: 0.4562 Eval Acc: 0.8515 (LR: 0.00100000)
[2025-06-12 10:44:50,394]: [ResNet18_parametrized_relu] Epoch: 013 Train Loss: 0.3849 Train Acc: 0.8671 Eval Loss: 0.4378 Eval Acc: 0.8537 (LR: 0.00100000)
[2025-06-12 10:46:36,705]: [ResNet18_parametrized_relu] Epoch: 014 Train Loss: 0.3714 Train Acc: 0.8710 Eval Loss: 0.4614 Eval Acc: 0.8482 (LR: 0.00100000)
[2025-06-12 10:48:22,826]: [ResNet18_parametrized_relu] Epoch: 015 Train Loss: 0.3548 Train Acc: 0.8785 Eval Loss: 0.5044 Eval Acc: 0.8345 (LR: 0.00100000)
[2025-06-12 10:50:09,074]: [ResNet18_parametrized_relu] Epoch: 016 Train Loss: 0.3414 Train Acc: 0.8817 Eval Loss: 0.3896 Eval Acc: 0.8702 (LR: 0.00100000)
[2025-06-12 10:51:55,303]: [ResNet18_parametrized_relu] Epoch: 017 Train Loss: 0.3264 Train Acc: 0.8877 Eval Loss: 0.5373 Eval Acc: 0.8398 (LR: 0.00100000)
[2025-06-12 10:53:41,503]: [ResNet18_parametrized_relu] Epoch: 018 Train Loss: 0.3147 Train Acc: 0.8928 Eval Loss: 0.4419 Eval Acc: 0.8608 (LR: 0.00100000)
[2025-06-12 10:55:27,737]: [ResNet18_parametrized_relu] Epoch: 019 Train Loss: 0.3066 Train Acc: 0.8945 Eval Loss: 0.3543 Eval Acc: 0.8834 (LR: 0.00100000)
[2025-06-12 10:57:14,838]: [ResNet18_parametrized_relu] Epoch: 020 Train Loss: 0.2925 Train Acc: 0.8987 Eval Loss: 0.3964 Eval Acc: 0.8790 (LR: 0.00100000)
[2025-06-12 10:59:00,959]: [ResNet18_parametrized_relu] Epoch: 021 Train Loss: 0.2863 Train Acc: 0.9009 Eval Loss: 0.3535 Eval Acc: 0.8854 (LR: 0.00100000)
[2025-06-12 11:00:47,162]: [ResNet18_parametrized_relu] Epoch: 022 Train Loss: 0.2831 Train Acc: 0.9014 Eval Loss: 0.3612 Eval Acc: 0.8818 (LR: 0.00100000)
[2025-06-12 11:02:33,342]: [ResNet18_parametrized_relu] Epoch: 023 Train Loss: 0.2717 Train Acc: 0.9063 Eval Loss: 0.3374 Eval Acc: 0.8905 (LR: 0.00100000)
[2025-06-12 11:04:19,368]: [ResNet18_parametrized_relu] Epoch: 024 Train Loss: 0.2644 Train Acc: 0.9101 Eval Loss: 0.3488 Eval Acc: 0.8869 (LR: 0.00100000)
[2025-06-12 11:06:05,427]: [ResNet18_parametrized_relu] Epoch: 025 Train Loss: 0.2556 Train Acc: 0.9114 Eval Loss: 0.4538 Eval Acc: 0.8650 (LR: 0.00100000)
[2025-06-12 11:07:51,442]: [ResNet18_parametrized_relu] Epoch: 026 Train Loss: 0.2475 Train Acc: 0.9147 Eval Loss: 0.3686 Eval Acc: 0.8860 (LR: 0.00100000)
[2025-06-12 11:09:37,470]: [ResNet18_parametrized_relu] Epoch: 027 Train Loss: 0.2463 Train Acc: 0.9155 Eval Loss: 0.3407 Eval Acc: 0.8931 (LR: 0.00100000)
[2025-06-12 11:11:23,453]: [ResNet18_parametrized_relu] Epoch: 028 Train Loss: 0.2402 Train Acc: 0.9153 Eval Loss: 0.3680 Eval Acc: 0.8828 (LR: 0.00100000)
[2025-06-12 11:13:09,434]: [ResNet18_parametrized_relu] Epoch: 029 Train Loss: 0.2393 Train Acc: 0.9169 Eval Loss: 0.3125 Eval Acc: 0.8983 (LR: 0.00100000)
[2025-06-12 11:14:55,683]: [ResNet18_parametrized_relu] Epoch: 030 Train Loss: 0.2297 Train Acc: 0.9214 Eval Loss: 0.3645 Eval Acc: 0.8875 (LR: 0.00100000)
[2025-06-12 11:16:41,737]: [ResNet18_parametrized_relu] Epoch: 031 Train Loss: 0.2243 Train Acc: 0.9232 Eval Loss: 0.3504 Eval Acc: 0.8906 (LR: 0.00100000)
[2025-06-12 11:18:27,937]: [ResNet18_parametrized_relu] Epoch: 032 Train Loss: 0.2198 Train Acc: 0.9241 Eval Loss: 0.3284 Eval Acc: 0.8929 (LR: 0.00100000)
[2025-06-12 11:20:14,254]: [ResNet18_parametrized_relu] Epoch: 033 Train Loss: 0.2139 Train Acc: 0.9249 Eval Loss: 0.3442 Eval Acc: 0.8967 (LR: 0.00100000)
[2025-06-12 11:22:00,494]: [ResNet18_parametrized_relu] Epoch: 034 Train Loss: 0.2157 Train Acc: 0.9256 Eval Loss: 0.3297 Eval Acc: 0.8964 (LR: 0.00100000)
[2025-06-12 11:23:46,730]: [ResNet18_parametrized_relu] Epoch: 035 Train Loss: 0.2136 Train Acc: 0.9257 Eval Loss: 0.3425 Eval Acc: 0.8964 (LR: 0.00010000)
[2025-06-12 11:25:32,936]: [ResNet18_parametrized_relu] Epoch: 036 Train Loss: 0.1358 Train Acc: 0.9534 Eval Loss: 0.2326 Eval Acc: 0.9259 (LR: 0.00010000)
[2025-06-12 11:27:19,193]: [ResNet18_parametrized_relu] Epoch: 037 Train Loss: 0.1082 Train Acc: 0.9635 Eval Loss: 0.2333 Eval Acc: 0.9297 (LR: 0.00010000)
[2025-06-12 11:29:05,534]: [ResNet18_parametrized_relu] Epoch: 038 Train Loss: 0.0976 Train Acc: 0.9667 Eval Loss: 0.2366 Eval Acc: 0.9313 (LR: 0.00010000)
[2025-06-12 11:30:51,793]: [ResNet18_parametrized_relu] Epoch: 039 Train Loss: 0.0895 Train Acc: 0.9687 Eval Loss: 0.2374 Eval Acc: 0.9322 (LR: 0.00010000)
[2025-06-12 11:32:39,402]: [ResNet18_parametrized_relu] Epoch: 040 Train Loss: 0.0843 Train Acc: 0.9708 Eval Loss: 0.2464 Eval Acc: 0.9295 (LR: 0.00010000)
[2025-06-12 11:34:26,105]: [ResNet18_parametrized_relu] Epoch: 041 Train Loss: 0.0812 Train Acc: 0.9721 Eval Loss: 0.2477 Eval Acc: 0.9301 (LR: 0.00010000)
[2025-06-12 11:36:12,285]: [ResNet18_parametrized_relu] Epoch: 042 Train Loss: 0.0751 Train Acc: 0.9739 Eval Loss: 0.2456 Eval Acc: 0.9319 (LR: 0.00001000)
[2025-06-12 11:37:58,571]: [ResNet18_parametrized_relu] Epoch: 043 Train Loss: 0.0678 Train Acc: 0.9774 Eval Loss: 0.2431 Eval Acc: 0.9330 (LR: 0.00001000)
[2025-06-12 11:39:45,044]: [ResNet18_parametrized_relu] Epoch: 044 Train Loss: 0.0650 Train Acc: 0.9784 Eval Loss: 0.2416 Eval Acc: 0.9344 (LR: 0.00001000)
[2025-06-12 11:41:31,272]: [ResNet18_parametrized_relu] Epoch: 045 Train Loss: 0.0633 Train Acc: 0.9787 Eval Loss: 0.2441 Eval Acc: 0.9331 (LR: 0.00001000)
[2025-06-12 11:43:17,526]: [ResNet18_parametrized_relu] Epoch: 046 Train Loss: 0.0606 Train Acc: 0.9795 Eval Loss: 0.2421 Eval Acc: 0.9344 (LR: 0.00001000)
[2025-06-12 11:45:03,805]: [ResNet18_parametrized_relu] Epoch: 047 Train Loss: 0.0608 Train Acc: 0.9795 Eval Loss: 0.2436 Eval Acc: 0.9343 (LR: 0.00001000)
[2025-06-12 11:46:50,038]: [ResNet18_parametrized_relu] Epoch: 048 Train Loss: 0.0605 Train Acc: 0.9789 Eval Loss: 0.2432 Eval Acc: 0.9344 (LR: 0.00000100)
[2025-06-12 11:48:36,313]: [ResNet18_parametrized_relu] Epoch: 049 Train Loss: 0.0575 Train Acc: 0.9804 Eval Loss: 0.2429 Eval Acc: 0.9343 (LR: 0.00000100)
[2025-06-12 11:50:22,506]: [ResNet18_parametrized_relu] Epoch: 050 Train Loss: 0.0577 Train Acc: 0.9809 Eval Loss: 0.2419 Eval Acc: 0.9348 (LR: 0.00000100)
[2025-06-12 11:52:08,787]: [ResNet18_parametrized_relu] Epoch: 051 Train Loss: 0.0579 Train Acc: 0.9804 Eval Loss: 0.2414 Eval Acc: 0.9352 (LR: 0.00000100)
[2025-06-12 11:53:55,017]: [ResNet18_parametrized_relu] Epoch: 052 Train Loss: 0.0597 Train Acc: 0.9799 Eval Loss: 0.2426 Eval Acc: 0.9352 (LR: 0.00000100)
[2025-06-12 11:55:41,251]: [ResNet18_parametrized_relu] Epoch: 053 Train Loss: 0.0590 Train Acc: 0.9795 Eval Loss: 0.2413 Eval Acc: 0.9345 (LR: 0.00000100)
[2025-06-12 11:57:27,340]: [ResNet18_parametrized_relu] Epoch: 054 Train Loss: 0.0583 Train Acc: 0.9799 Eval Loss: 0.2425 Eval Acc: 0.9353 (LR: 0.00000010)
[2025-06-12 11:59:13,557]: [ResNet18_parametrized_relu] Epoch: 055 Train Loss: 0.0597 Train Acc: 0.9790 Eval Loss: 0.2432 Eval Acc: 0.9346 (LR: 0.00000010)
[2025-06-12 12:00:59,475]: [ResNet18_parametrized_relu] Epoch: 056 Train Loss: 0.0582 Train Acc: 0.9810 Eval Loss: 0.2439 Eval Acc: 0.9344 (LR: 0.00000010)
[2025-06-12 12:02:45,705]: [ResNet18_parametrized_relu] Epoch: 057 Train Loss: 0.0601 Train Acc: 0.9792 Eval Loss: 0.2454 Eval Acc: 0.9336 (LR: 0.00000010)
[2025-06-12 12:04:31,754]: [ResNet18_parametrized_relu] Epoch: 058 Train Loss: 0.0566 Train Acc: 0.9808 Eval Loss: 0.2435 Eval Acc: 0.9341 (LR: 0.00000010)
[2025-06-12 12:06:17,996]: [ResNet18_parametrized_relu] Epoch: 059 Train Loss: 0.0572 Train Acc: 0.9800 Eval Loss: 0.2438 Eval Acc: 0.9337 (LR: 0.00000010)
[2025-06-12 12:08:05,048]: [ResNet18_parametrized_relu] Epoch: 060 Train Loss: 0.0572 Train Acc: 0.9812 Eval Loss: 0.2452 Eval Acc: 0.9335 (LR: 0.00000010)
[2025-06-12 12:09:51,109]: [ResNet18_parametrized_relu] Epoch: 061 Train Loss: 0.0585 Train Acc: 0.9803 Eval Loss: 0.2439 Eval Acc: 0.9341 (LR: 0.00000010)
[2025-06-12 12:11:37,173]: [ResNet18_parametrized_relu] Epoch: 062 Train Loss: 0.0591 Train Acc: 0.9797 Eval Loss: 0.2456 Eval Acc: 0.9340 (LR: 0.00000010)
[2025-06-12 12:13:23,392]: [ResNet18_parametrized_relu] Epoch: 063 Train Loss: 0.0611 Train Acc: 0.9788 Eval Loss: 0.2442 Eval Acc: 0.9347 (LR: 0.00000010)
[2025-06-12 12:15:09,609]: [ResNet18_parametrized_relu] Epoch: 064 Train Loss: 0.0578 Train Acc: 0.9805 Eval Loss: 0.2447 Eval Acc: 0.9338 (LR: 0.00000010)
[2025-06-12 12:16:55,479]: [ResNet18_parametrized_relu] Epoch: 065 Train Loss: 0.0566 Train Acc: 0.9811 Eval Loss: 0.2435 Eval Acc: 0.9346 (LR: 0.00000010)
[2025-06-12 12:18:41,590]: [ResNet18_parametrized_relu] Epoch: 066 Train Loss: 0.0558 Train Acc: 0.9810 Eval Loss: 0.2448 Eval Acc: 0.9355 (LR: 0.00000010)
[2025-06-12 12:20:27,819]: [ResNet18_parametrized_relu] Epoch: 067 Train Loss: 0.0578 Train Acc: 0.9804 Eval Loss: 0.2421 Eval Acc: 0.9343 (LR: 0.00000010)
[2025-06-12 12:22:14,047]: [ResNet18_parametrized_relu] Epoch: 068 Train Loss: 0.0573 Train Acc: 0.9800 Eval Loss: 0.2429 Eval Acc: 0.9343 (LR: 0.00000010)
[2025-06-12 12:24:00,334]: [ResNet18_parametrized_relu] Epoch: 069 Train Loss: 0.0591 Train Acc: 0.9801 Eval Loss: 0.2422 Eval Acc: 0.9348 (LR: 0.00000010)
[2025-06-12 12:25:46,586]: [ResNet18_parametrized_relu] Epoch: 070 Train Loss: 0.0568 Train Acc: 0.9808 Eval Loss: 0.2419 Eval Acc: 0.9351 (LR: 0.00000010)
[2025-06-12 12:27:32,616]: [ResNet18_parametrized_relu] Epoch: 071 Train Loss: 0.0571 Train Acc: 0.9810 Eval Loss: 0.2454 Eval Acc: 0.9339 (LR: 0.00000010)
[2025-06-12 12:29:18,961]: [ResNet18_parametrized_relu] Epoch: 072 Train Loss: 0.0583 Train Acc: 0.9793 Eval Loss: 0.2440 Eval Acc: 0.9352 (LR: 0.00000010)
[2025-06-12 12:31:05,346]: [ResNet18_parametrized_relu] Epoch: 073 Train Loss: 0.0587 Train Acc: 0.9794 Eval Loss: 0.2428 Eval Acc: 0.9339 (LR: 0.00000010)
[2025-06-12 12:32:51,789]: [ResNet18_parametrized_relu] Epoch: 074 Train Loss: 0.0567 Train Acc: 0.9809 Eval Loss: 0.2437 Eval Acc: 0.9348 (LR: 0.00000010)
[2025-06-12 12:34:38,047]: [ResNet18_parametrized_relu] Epoch: 075 Train Loss: 0.0607 Train Acc: 0.9796 Eval Loss: 0.2448 Eval Acc: 0.9349 (LR: 0.00000010)
[2025-06-12 12:36:24,098]: [ResNet18_parametrized_relu] Epoch: 076 Train Loss: 0.0569 Train Acc: 0.9804 Eval Loss: 0.2432 Eval Acc: 0.9338 (LR: 0.00000010)
[2025-06-12 12:38:10,112]: [ResNet18_parametrized_relu] Epoch: 077 Train Loss: 0.0596 Train Acc: 0.9800 Eval Loss: 0.2408 Eval Acc: 0.9346 (LR: 0.00000010)
[2025-06-12 12:39:55,994]: [ResNet18_parametrized_relu] Epoch: 078 Train Loss: 0.0590 Train Acc: 0.9795 Eval Loss: 0.2426 Eval Acc: 0.9345 (LR: 0.00000010)
[2025-06-12 12:41:42,247]: [ResNet18_parametrized_relu] Epoch: 079 Train Loss: 0.0578 Train Acc: 0.9804 Eval Loss: 0.2414 Eval Acc: 0.9355 (LR: 0.00000010)
[2025-06-12 12:43:28,956]: [ResNet18_parametrized_relu] Epoch: 080 Train Loss: 0.0584 Train Acc: 0.9806 Eval Loss: 0.2431 Eval Acc: 0.9345 (LR: 0.00000010)
[2025-06-12 12:45:14,978]: [ResNet18_parametrized_relu] Epoch: 081 Train Loss: 0.0595 Train Acc: 0.9793 Eval Loss: 0.2439 Eval Acc: 0.9342 (LR: 0.00000010)
[2025-06-12 12:47:01,021]: [ResNet18_parametrized_relu] Epoch: 082 Train Loss: 0.0586 Train Acc: 0.9804 Eval Loss: 0.2440 Eval Acc: 0.9339 (LR: 0.00000010)
[2025-06-12 12:48:47,118]: [ResNet18_parametrized_relu] Epoch: 083 Train Loss: 0.0588 Train Acc: 0.9795 Eval Loss: 0.2438 Eval Acc: 0.9339 (LR: 0.00000010)
[2025-06-12 12:50:33,141]: [ResNet18_parametrized_relu] Epoch: 084 Train Loss: 0.0582 Train Acc: 0.9798 Eval Loss: 0.2437 Eval Acc: 0.9343 (LR: 0.00000010)
[2025-06-12 12:52:19,014]: [ResNet18_parametrized_relu] Epoch: 085 Train Loss: 0.0583 Train Acc: 0.9802 Eval Loss: 0.2454 Eval Acc: 0.9344 (LR: 0.00000010)
[2025-06-12 12:54:05,073]: [ResNet18_parametrized_relu] Epoch: 086 Train Loss: 0.0592 Train Acc: 0.9801 Eval Loss: 0.2429 Eval Acc: 0.9346 (LR: 0.00000010)
[2025-06-12 12:55:51,287]: [ResNet18_parametrized_relu] Epoch: 087 Train Loss: 0.0565 Train Acc: 0.9809 Eval Loss: 0.2428 Eval Acc: 0.9341 (LR: 0.00000010)
[2025-06-12 12:57:37,162]: [ResNet18_parametrized_relu] Epoch: 088 Train Loss: 0.0589 Train Acc: 0.9797 Eval Loss: 0.2419 Eval Acc: 0.9351 (LR: 0.00000010)
[2025-06-12 12:59:23,063]: [ResNet18_parametrized_relu] Epoch: 089 Train Loss: 0.0569 Train Acc: 0.9805 Eval Loss: 0.2422 Eval Acc: 0.9342 (LR: 0.00000010)
[2025-06-12 13:01:09,288]: [ResNet18_parametrized_relu] Epoch: 090 Train Loss: 0.0582 Train Acc: 0.9804 Eval Loss: 0.2441 Eval Acc: 0.9343 (LR: 0.00000010)
[2025-06-12 13:02:55,502]: [ResNet18_parametrized_relu] Epoch: 091 Train Loss: 0.0573 Train Acc: 0.9810 Eval Loss: 0.2449 Eval Acc: 0.9338 (LR: 0.00000010)
[2025-06-12 13:04:41,710]: [ResNet18_parametrized_relu] Epoch: 092 Train Loss: 0.0592 Train Acc: 0.9800 Eval Loss: 0.2454 Eval Acc: 0.9331 (LR: 0.00000010)
[2025-06-12 13:06:27,933]: [ResNet18_parametrized_relu] Epoch: 093 Train Loss: 0.0603 Train Acc: 0.9797 Eval Loss: 0.2436 Eval Acc: 0.9344 (LR: 0.00000010)
[2025-06-12 13:08:14,143]: [ResNet18_parametrized_relu] Epoch: 094 Train Loss: 0.0580 Train Acc: 0.9802 Eval Loss: 0.2434 Eval Acc: 0.9357 (LR: 0.00000010)
[2025-06-12 13:10:00,365]: [ResNet18_parametrized_relu] Epoch: 095 Train Loss: 0.0586 Train Acc: 0.9799 Eval Loss: 0.2435 Eval Acc: 0.9339 (LR: 0.00000010)
[2025-06-12 13:11:46,587]: [ResNet18_parametrized_relu] Epoch: 096 Train Loss: 0.0568 Train Acc: 0.9814 Eval Loss: 0.2435 Eval Acc: 0.9343 (LR: 0.00000010)
[2025-06-12 13:13:32,788]: [ResNet18_parametrized_relu] Epoch: 097 Train Loss: 0.0585 Train Acc: 0.9801 Eval Loss: 0.2425 Eval Acc: 0.9336 (LR: 0.00000010)
[2025-06-12 13:15:19,158]: [ResNet18_parametrized_relu] Epoch: 098 Train Loss: 0.0574 Train Acc: 0.9812 Eval Loss: 0.2433 Eval Acc: 0.9344 (LR: 0.00000010)
[2025-06-12 13:17:05,386]: [ResNet18_parametrized_relu] Epoch: 099 Train Loss: 0.0580 Train Acc: 0.9800 Eval Loss: 0.2427 Eval Acc: 0.9343 (LR: 0.00000010)
[2025-06-12 13:18:52,281]: [ResNet18_parametrized_relu] Epoch: 100 Train Loss: 0.0579 Train Acc: 0.9807 Eval Loss: 0.2448 Eval Acc: 0.9343 (LR: 0.00000010)
[2025-06-12 13:18:52,281]: [ResNet18_parametrized_relu] Best Eval Accuracy: 0.9357
[2025-06-12 13:18:52,354]: 
Training of full-precision model finished!
[2025-06-12 13:18:52,354]: Model Architecture:
[2025-06-12 13:18:52,354]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-06-12 13:18:52,355]: 
Model Weights:
[2025-06-12 13:18:52,355]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-06-12 13:18:52,368]: Sample Values (25 elements): [-0.057403434067964554, -0.2110276073217392, -0.14984260499477386, -0.06300974637269974, 0.25336647033691406, 0.05276219919323921, -0.1921117901802063, -0.2211298644542694, -0.03293003886938095, -0.1727258265018463, 0.24778859317302704, 0.04907003417611122, -0.01577823795378208, -0.11409009248018265, 0.041575536131858826, -0.20492121577262878, 0.19594545662403107, -0.0201142355799675, 0.015699680894613266, -0.020025121048092842, -0.17695409059524536, 0.053649190813302994, -0.02131291851401329, 0.17128650844097137, -0.07886286824941635]
[2025-06-12 13:18:52,376]: Mean: -0.00133291
[2025-06-12 13:18:52,385]: Min: -0.41238472
[2025-06-12 13:18:52,386]: Max: 0.38110301
[2025-06-12 13:18:52,386]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-06-12 13:18:52,386]: Sample Values (25 elements): [0.9272122979164124, 0.7777516841888428, 0.5134421586990356, 0.6157809495925903, 0.6802189946174622, 0.9488819241523743, 0.6835405826568604, 0.5741692185401917, 0.8392475247383118, 0.9168457388877869, 0.5370544791221619, 0.6263384222984314, 0.8078025579452515, 0.5661144256591797, 0.5689472556114197, 0.7411901950836182, 0.8452535271644592, 0.6991331577301025, 0.5730986595153809, 0.9621822834014893, 0.7189117074012756, 0.7022349238395691, 0.48031118512153625, 0.8828855156898499, 0.7580682635307312]
[2025-06-12 13:18:52,386]: Mean: 0.68881238
[2025-06-12 13:18:52,387]: Min: 0.32071456
[2025-06-12 13:18:52,387]: Max: 1.19088471
[2025-06-12 13:18:52,387]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 13:18:52,387]: Sample Values (25 elements): [-0.08265665918588638, 0.023544229567050934, 0.029248835518956184, -0.06646200269460678, 0.07860768586397171, 0.009816460311412811, -0.08621439337730408, 0.07624128460884094, 0.015802018344402313, -0.025610342621803284, 0.01789257489144802, -0.01022728718817234, 0.018118159845471382, -0.006761439144611359, -0.031156504526734352, 0.019976068288087845, -0.07562469691038132, 0.06441392749547958, -0.050607796758413315, -0.061325009912252426, -0.016421480104327202, 0.0014349891571328044, 0.0035011640284210443, 0.10094039887189865, 0.062396980822086334]
[2025-06-12 13:18:52,388]: Mean: -0.00855509
[2025-06-12 13:18:52,388]: Min: -0.43877017
[2025-06-12 13:18:52,388]: Max: 0.35489297
[2025-06-12 13:18:52,388]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-06-12 13:18:52,388]: Sample Values (25 elements): [0.9584031105041504, 0.5792369246482849, 0.6510552167892456, 0.6841170191764832, 0.6476957201957703, 0.9448009133338928, 0.6112890839576721, 0.6506057977676392, 0.5199974179267883, 0.7094056010246277, 0.6168138980865479, 0.8020706176757812, 0.465511679649353, 0.8067975044250488, 0.7073894739151001, 0.6172778010368347, 0.5599754452705383, 0.7779766917228699, 0.557318925857544, 0.9475940465927124, 0.5404794216156006, 0.727989137172699, 0.4226140081882477, 0.6861013174057007, 0.6324199438095093]
[2025-06-12 13:18:52,388]: Mean: 0.69236207
[2025-06-12 13:18:52,389]: Min: 0.41370764
[2025-06-12 13:18:52,389]: Max: 1.01272893
[2025-06-12 13:18:52,389]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 13:18:52,389]: Sample Values (25 elements): [-0.030225815251469612, 0.09452428668737411, -0.02347940020263195, 0.07825808972120285, 0.0860014334321022, 0.0018585750367492437, 0.0061210100539028645, -0.005531441420316696, -0.016974953934550285, -0.013841544277966022, -0.01835261844098568, 0.0410156175494194, -0.1268039494752884, 0.06665085256099701, 0.10778505355119705, -0.0009853930678218603, 0.023513859137892723, -0.050691328942775726, -0.0424630306661129, -0.006155283190310001, 0.0047752889804542065, 0.005598935764282942, 0.009206926450133324, -0.02317766658961773, 0.026733146980404854]
[2025-06-12 13:18:52,390]: Mean: -0.00264231
[2025-06-12 13:18:52,390]: Min: -0.37536830
[2025-06-12 13:18:52,390]: Max: 0.42965677
[2025-06-12 13:18:52,390]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-06-12 13:18:52,390]: Sample Values (25 elements): [0.7623592615127563, 0.480207234621048, 0.8409579992294312, 0.5912061333656311, 0.6423842906951904, 0.9876301884651184, 0.3740050494670868, 0.6874818205833435, 0.6838454008102417, 0.6714768409729004, 0.5257466435432434, 0.7110803723335266, 0.720129132270813, 0.8107137680053711, 0.7152232527732849, 0.6409523487091064, 0.7912463545799255, 1.0166168212890625, 0.9985703825950623, 0.7178307175636292, 0.5780077576637268, 0.8382840156555176, 0.7689886689186096, 0.4696007966995239, 0.7403497695922852]
[2025-06-12 13:18:52,390]: Mean: 0.77853662
[2025-06-12 13:18:52,390]: Min: 0.37400505
[2025-06-12 13:18:52,391]: Max: 1.32681096
[2025-06-12 13:18:52,391]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 13:18:52,391]: Sample Values (25 elements): [0.04940416291356087, 0.05956422910094261, -0.033155154436826706, -0.06437701731920242, -0.01093856617808342, -0.007162040565162897, 0.006283532828092575, 0.07992017269134521, 0.016166741028428078, 0.0504666268825531, 0.03358026221394539, -0.0025221933610737324, -0.02668648026883602, 0.006701189558953047, -0.053381744772195816, -0.015081427060067654, -0.0009495086851529777, 0.023815589025616646, 0.009998633526265621, 0.017796611413359642, -0.0022532714065164328, 0.056749362498521805, 0.05150804668664932, 0.18793509900569916, -0.049320120364427567]
[2025-06-12 13:18:52,391]: Mean: -0.00544032
[2025-06-12 13:18:52,392]: Min: -0.40877891
[2025-06-12 13:18:52,392]: Max: 0.37694454
[2025-06-12 13:18:52,392]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-06-12 13:18:52,392]: Sample Values (25 elements): [1.067080020904541, 0.5977405905723572, 0.44174522161483765, 1.0346848964691162, 0.4826907515525818, 0.7093009352684021, 0.6054672598838806, 0.6799279451370239, 0.5291348099708557, 0.7692004442214966, 0.8422165513038635, 0.5863639116287231, 0.7507456541061401, 0.3894958198070526, 0.5987687706947327, 0.6373123526573181, 0.5115400552749634, 0.5528942942619324, 0.49176448583602905, 0.5033930540084839, 0.7938374876976013, 0.6969642639160156, 0.438130646944046, 0.26690250635147095, 0.7048906087875366]
[2025-06-12 13:18:52,392]: Mean: 0.63129288
[2025-06-12 13:18:52,392]: Min: 0.26690251
[2025-06-12 13:18:52,393]: Max: 1.15943205
[2025-06-12 13:18:52,393]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 13:18:52,393]: Sample Values (25 elements): [-0.03846525400876999, -0.013276176527142525, -0.05476570874452591, 0.015594255179166794, -0.008455886505544186, -0.03385826200246811, 0.015617883764207363, -0.03679842874407768, -0.016139840707182884, 0.035765327513217926, -0.09720712155103683, 0.06588628888130188, -0.01815043017268181, 0.0011903723934665322, 0.05485447868704796, 0.0016132863238453865, 0.012542452663183212, 0.10469203442335129, 0.0132687883451581, 6.358390237437561e-05, 0.03622882440686226, -0.08833833038806915, 0.08202460408210754, 0.008863505907356739, 0.061538346111774445]
[2025-06-12 13:18:52,393]: Mean: -0.00370016
[2025-06-12 13:18:52,393]: Min: -0.35626560
[2025-06-12 13:18:52,394]: Max: 0.29486489
[2025-06-12 13:18:52,394]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-06-12 13:18:52,394]: Sample Values (25 elements): [0.8158454895019531, 0.4801429212093353, 0.6183720231056213, 0.8426809310913086, 1.0320141315460205, 0.6174725890159607, 0.30587029457092285, 0.618131697177887, 0.6852734088897705, 1.0721843242645264, 0.8280209898948669, 0.7216156721115112, 0.5961499214172363, 0.5647172927856445, 0.8098251223564148, 0.544083833694458, 0.6554109454154968, 0.7276631593704224, 0.7287404537200928, 0.5659198760986328, 0.7345988750457764, 0.5181849002838135, 0.5365030169487, 0.5613682270050049, 0.7310166954994202]
[2025-06-12 13:18:52,394]: Mean: 0.69137514
[2025-06-12 13:18:52,394]: Min: 0.30587029
[2025-06-12 13:18:52,394]: Max: 1.09100914
[2025-06-12 13:18:52,394]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-06-12 13:18:52,395]: Sample Values (25 elements): [0.0011035875650122762, 0.048068203032016754, -0.02245606854557991, -0.03582108020782471, 0.03061421401798725, 0.0073054758831858635, 0.01882438361644745, 0.028720282018184662, -0.05941405147314072, 0.019283408299088478, -0.007818722166121006, 0.09350728988647461, 0.029251379892230034, 0.05774621665477753, 0.12042295187711716, 0.014226079918444157, -0.011710354126989841, 0.049626272171735764, -0.015292087569832802, -0.04088420793414116, -0.08048132061958313, 0.03877883404493332, -0.04912383109331131, -0.11844030022621155, -0.09538178145885468]
[2025-06-12 13:18:52,395]: Mean: -0.00265392
[2025-06-12 13:18:52,396]: Min: -0.37903717
[2025-06-12 13:18:52,396]: Max: 0.33589536
[2025-06-12 13:18:52,396]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-06-12 13:18:52,396]: Sample Values (25 elements): [0.5363612771034241, 0.604124903678894, 0.4493461549282074, 0.5987054705619812, 0.38627874851226807, 0.37778544425964355, 0.3594856858253479, 0.5762007832527161, 0.7763549089431763, 0.3303203284740448, 0.8626277446746826, 0.7013410329818726, 0.6260618567466736, 0.17982685565948486, 0.5143829584121704, 0.6886284351348877, 0.5728675127029419, 0.8804962038993835, 0.6402349472045898, 0.6573609709739685, 0.5289971828460693, 0.6940829753875732, 0.11135450750589371, 0.5189899206161499, 0.3896728456020355]
[2025-06-12 13:18:52,396]: Mean: 0.53699207
[2025-06-12 13:18:52,396]: Min: 0.02525636
[2025-06-12 13:18:52,396]: Max: 0.88049620
[2025-06-12 13:18:52,397]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-12 13:18:52,398]: Sample Values (25 elements): [-0.021332046017050743, -0.0193491168320179, -0.045410871505737305, 0.05103694275021553, -0.04729696735739708, 0.009912780486047268, 0.013207405805587769, 0.1382547914981842, -0.00904831849038601, 0.03972405195236206, 0.03866087272763252, -0.018437569960951805, 0.04075309634208679, -0.07215603440999985, 0.0770246759057045, -0.04873921349644661, 0.012341481633484364, 0.05600423738360405, -0.03879189491271973, 0.017625249922275543, -0.0023725389037281275, 0.018517863005399704, 0.163551926612854, -0.026772763580083847, -0.02635369449853897]
[2025-06-12 13:18:52,398]: Mean: -0.00317853
[2025-06-12 13:18:52,398]: Min: -0.27198330
[2025-06-12 13:18:52,398]: Max: 0.28165454
[2025-06-12 13:18:52,398]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-06-12 13:18:52,399]: Sample Values (25 elements): [0.726392924785614, 0.6042314171791077, 0.662177324295044, 0.6863420009613037, 0.48623475432395935, 0.5147891640663147, 0.5954651236534119, 0.7348807454109192, 0.4911232888698578, 0.48211801052093506, 0.5708728432655334, 0.616992712020874, 0.4724222719669342, 0.7377695441246033, 0.5939332842826843, 0.5587497353553772, 0.6316410899162292, 0.5997000336647034, 0.7714813351631165, 0.7188013195991516, 0.47959429025650024, 0.6964386105537415, 0.8492785096168518, 0.5470477342605591, 0.7529911398887634]
[2025-06-12 13:18:52,399]: Mean: 0.64134741
[2025-06-12 13:18:52,399]: Min: 0.21731184
[2025-06-12 13:18:52,399]: Max: 0.97611278
[2025-06-12 13:18:52,399]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-06-12 13:18:52,399]: Sample Values (25 elements): [-0.015359117649495602, -0.04457753151655197, 0.14580407738685608, -0.07642703503370285, 0.19072584807872772, 0.008528655394911766, -0.018397629261016846, 0.0637737289071083, -0.06132141128182411, -0.006381538696587086, 0.06528209149837494, 0.025239672511816025, -0.12999437749385834, -0.03664146363735199, -0.27670609951019287, 0.09766926616430283, -0.05841463431715965, 0.06353935599327087, 0.018461057916283607, -0.021327435970306396, 0.09017825126647949, 0.13787221908569336, -0.012696197256445885, 0.0341646671295166, -0.05410271883010864]
[2025-06-12 13:18:52,400]: Mean: -0.00298101
[2025-06-12 13:18:52,400]: Min: -0.36944079
[2025-06-12 13:18:52,400]: Max: 0.46315685
[2025-06-12 13:18:52,400]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-06-12 13:18:52,400]: Sample Values (25 elements): [0.5193139910697937, 0.7172474265098572, 0.3870289623737335, 0.5803892016410828, 0.22924301028251648, 0.37175410985946655, 0.41480934619903564, 0.5320449471473694, 0.5714545249938965, 0.597657322883606, 0.5277025699615479, 0.4969882369041443, 0.5309872627258301, 0.5187062621116638, 0.5875769853591919, 0.7219345569610596, 0.3602267801761627, 0.4024226665496826, 0.514109194278717, 0.6266975998878479, 0.6017044186592102, 0.4429696202278137, 0.5048213005065918, 0.6517934203147888, 0.5638507008552551]
[2025-06-12 13:18:52,400]: Mean: 0.52479321
[2025-06-12 13:18:52,401]: Min: 0.20221981
[2025-06-12 13:18:52,401]: Max: 0.85155040
[2025-06-12 13:18:52,401]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-12 13:18:52,402]: Sample Values (25 elements): [-0.06529320031404495, -0.05195660516619682, -0.03043142706155777, -1.1803274015727977e-13, 0.044798847287893295, 0.08273070305585861, 0.0035290434025228024, -0.11072485893964767, -0.11538401246070862, -1.2032046470267233e-09, -0.004082237835973501, -0.024783749133348465, -0.07850974798202515, -0.059768181294202805, 0.01967575214803219, 0.009749430231750011, -0.05851475149393082, 0.04425767809152603, 0.004498657304793596, 0.00011577795521588996, -0.15110763907432556, -0.1004767119884491, 0.008553578518331051, 0.06213202700018883, 4.5638819112792106e-11]
[2025-06-12 13:18:52,402]: Mean: -0.00381107
[2025-06-12 13:18:52,402]: Min: -0.27656674
[2025-06-12 13:18:52,403]: Max: 0.26771140
[2025-06-12 13:18:52,403]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-06-12 13:18:52,403]: Sample Values (25 elements): [0.6334719657897949, 0.5815212726593018, 0.7193036079406738, 0.6089332103729248, 0.5463932752609253, 0.7200555801391602, 0.3803378939628601, 0.643275260925293, 0.5159608125686646, 0.5502350330352783, 0.393716961145401, 0.03402353450655937, 0.11263088881969452, 0.5080783367156982, 0.5002222657203674, 0.5063967108726501, 0.43079641461372375, 0.38747933506965637, 0.584382176399231, 0.6016502380371094, 0.4872608780860901, 0.8341461420059204, 0.4051881730556488, 0.5707995891571045, 0.7190007567405701]
[2025-06-12 13:18:52,403]: Mean: 0.49118316
[2025-06-12 13:18:52,403]: Min: 0.00229663
[2025-06-12 13:18:52,403]: Max: 0.84734613
[2025-06-12 13:18:52,403]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-12 13:18:52,405]: Sample Values (25 elements): [0.03875831887125969, -0.028113100677728653, -0.02164609357714653, -0.0005221778410486877, 0.031626809388399124, -0.11009912937879562, 0.012094523757696152, -0.04797257110476494, -0.019767940044403076, -0.05990801379084587, -0.01401077676564455, 0.07662901282310486, -0.011300559155642986, 0.0702965259552002, 0.07236088067293167, -0.03306659683585167, 0.022233976051211357, -0.05511658638715744, -1.5627396129478334e-34, -0.11171027272939682, 0.09970984607934952, -0.03132084384560585, -0.06485722213983536, 0.07002145797014236, 0.009726699441671371]
[2025-06-12 13:18:52,405]: Mean: -0.00290512
[2025-06-12 13:18:52,405]: Min: -0.23766582
[2025-06-12 13:18:52,405]: Max: 0.29389644
[2025-06-12 13:18:52,405]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-06-12 13:18:52,406]: Sample Values (25 elements): [0.3614237308502197, 0.5799340605735779, 0.580511212348938, 0.47193899750709534, 0.6249392032623291, 0.2755078077316284, 0.48919597268104553, 0.3599490225315094, 0.48812738060951233, 0.5988192558288574, 0.705160915851593, 0.4690544903278351, 0.5240849256515503, 0.6109271049499512, 0.6345122456550598, 0.400751531124115, 0.4154040813446045, 0.5229609608650208, 0.48637983202934265, 0.5294422507286072, 0.4386144280433655, 0.6882129907608032, 0.739042341709137, 0.6405273675918579, 0.5499907732009888]
[2025-06-12 13:18:52,406]: Mean: 0.49869925
[2025-06-12 13:18:52,406]: Min: 0.06683627
[2025-06-12 13:18:52,406]: Max: 0.81680167
[2025-06-12 13:18:52,406]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-06-12 13:18:52,409]: Sample Values (25 elements): [-0.0018423886504024267, -0.00033119379077106714, -0.004758212249726057, -0.039900775998830795, -6.629469595509363e-08, -0.02557390183210373, 0.0028263095300644636, -0.0039628613740205765, 5.316763732339829e-29, 0.012479716911911964, 0.07285961508750916, -0.002447242848575115, 0.002965156454592943, -5.595636801136727e-13, -0.012196466326713562, -0.008645583875477314, 0.06574524939060211, -2.15357379264458e-33, 4.92318189471238e-41, 0.015152896754443645, 0.00041147469892166555, -0.07694657146930695, -0.02511267550289631, 0.0045920563861727715, 0.0025860921014100313]
[2025-06-12 13:18:52,409]: Mean: -0.00131193
[2025-06-12 13:18:52,409]: Min: -0.24762893
[2025-06-12 13:18:52,409]: Max: 0.28754711
[2025-06-12 13:18:52,409]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-06-12 13:18:52,410]: Sample Values (25 elements): [0.2211741954088211, 0.1690177619457245, 0.4707520008087158, 1.5174782264093523e-10, 0.7468227744102478, 0.31874144077301025, 0.5093303918838501, 0.6999413967132568, 0.499554842710495, 2.1512021330938325e-11, 0.5013181567192078, 0.09153799712657928, 0.02153387852013111, 0.5350218415260315, 0.03890984505414963, 0.46635591983795166, 0.0007612954941578209, 0.13025841116905212, 0.02954818867146969, 0.00846637599170208, 0.5794866681098938, 0.45800334215164185, 0.25862857699394226, 0.6100867390632629, 0.3298514783382416]
[2025-06-12 13:18:52,410]: Mean: 0.33462363
[2025-06-12 13:18:52,410]: Min: 0.00000000
[2025-06-12 13:18:52,410]: Max: 0.74682277
[2025-06-12 13:18:52,410]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-12 13:18:52,415]: Sample Values (25 elements): [0.0795656219124794, -0.00950319692492485, 9.152234060324817e-24, 0.013915041461586952, 0.0015178766334429383, 0.004622028209269047, -0.06852973252534866, 2.4457434186366884e-35, 0.10274718701839447, 0.001565355807542801, -0.05562390014529228, 0.09351896494626999, 0.03746028617024422, 0.0027524048928171396, 0.016205793246626854, -0.0025939124170690775, -4.292223457014188e-05, 1.395983574667775e-36, -4.92430293348384e-41, 0.03063674084842205, 0.004701283294707537, 2.2476922822534107e-05, -1.2862145948035271e-27, 0.0020961982663720846, 1.2916049641006897e-21]
[2025-06-12 13:18:52,415]: Mean: -0.00100400
[2025-06-12 13:18:52,415]: Min: -0.25931147
[2025-06-12 13:18:52,416]: Max: 0.36871344
[2025-06-12 13:18:52,416]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-06-12 13:18:52,416]: Sample Values (25 elements): [0.616277277469635, 0.0023625316098332405, 0.07225746661424637, 0.5505739450454712, 0.3981096148490906, 0.40349090099334717, 0.6232069134712219, 0.3994937539100647, 0.001868735533207655, 0.46799612045288086, 0.42179980874061584, 0.4564445912837982, 0.489114910364151, 0.6566298604011536, 0.38928112387657166, 0.089805006980896, 0.43031585216522217, 0.3182198703289032, 0.5501086115837097, 0.4117635190486908, 0.705606997013092, 0.5033041834831238, 0.37446585297584534, 0.5265885591506958, 0.4555051028728485]
[2025-06-12 13:18:52,416]: Mean: 0.44112629
[2025-06-12 13:18:52,416]: Min: -0.00177242
[2025-06-12 13:18:52,416]: Max: 0.89810473
[2025-06-12 13:18:52,416]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-06-12 13:18:52,417]: Sample Values (25 elements): [0.025865130126476288, 0.04534733295440674, -0.2110186070203781, 0.06001583859324455, 0.09625737369060516, 0.08514474332332611, 0.028422726318240166, -0.007916207425296307, 0.009668630547821522, -0.011421558447182178, 0.07443007826805115, 0.023921428248286247, -0.09821885079145432, -0.048994068056344986, 0.08399772644042969, -0.015477202832698822, 0.07529880851507187, 0.19179508090019226, 5.828519075096056e-17, -0.014172236435115337, -0.009122834540903568, 0.026175042614340782, -0.035219598561525345, -0.09910033643245697, -0.02942158281803131]
[2025-06-12 13:18:52,417]: Mean: -0.00400786
[2025-06-12 13:18:52,417]: Min: -0.28620651
[2025-06-12 13:18:52,417]: Max: 0.31983313
[2025-06-12 13:18:52,417]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-06-12 13:18:52,418]: Sample Values (25 elements): [0.29766637086868286, 0.06947852671146393, 0.4049471914768219, 0.1697889119386673, 0.457287460565567, 0.34520936012268066, 0.3298572301864624, 0.008116638287901878, 0.45471739768981934, 0.26914113759994507, 0.0006123930215835571, 0.4762669801712036, 0.22746622562408447, 1.972386698412265e-10, 0.3286953866481781, 0.2528403103351593, 0.14962904155254364, 0.3709271252155304, 0.3524722754955292, 0.3301253020763397, 2.288233824643271e-09, 0.0031617418862879276, 0.536137580871582, 0.4510524570941925, 0.1218927651643753]
[2025-06-12 13:18:52,418]: Mean: 0.29190052
[2025-06-12 13:18:52,418]: Min: -0.00000000
[2025-06-12 13:18:52,418]: Max: 0.56723607
[2025-06-12 13:18:52,418]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-12 13:18:52,425]: Sample Values (25 elements): [0.015615328215062618, 4.943220462752225e-41, 0.04658167064189911, -0.06975135952234268, 0.02566245011985302, -4.913933324847836e-41, 3.101734182564542e-05, -4.915054363619296e-41, -1.2072162225656681e-28, 0.0010188771411776543, 4.922200985787352e-41, 4.918417479933675e-41, 4.933691633194816e-41, 4.904965014676157e-41, -4.922341115633785e-41, 0.01055449154227972, 4.933271243655518e-41, 4.904965014676157e-41, 4.95709331754904e-41, -4.934952801812708e-41, 0.08259179443120956, 9.325953179972808e-35, 5.048037587883721e-41, -0.004965309984982014, -0.04148036614060402]
[2025-06-12 13:18:52,425]: Mean: -0.00099153
[2025-06-12 13:18:52,425]: Min: -0.27919769
[2025-06-12 13:18:52,425]: Max: 0.33605331
[2025-06-12 13:18:52,425]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-06-12 13:18:52,425]: Sample Values (25 elements): [0.04303096979856491, 0.4097372591495514, 0.38294175267219543, 0.05956798046827316, 0.01410568505525589, 3.067131183343008e-05, 8.440038641310821e-07, 0.3869958221912384, 0.3603959381580353, 3.549275296513054e-21, 0.00986937340348959, -4.870238327209231e-12, 0.6049703359603882, 0.5537683367729187, 3.318329836713048e-14, 0.0069353836588561535, 0.007808485068380833, 0.45452556014060974, 0.37802985310554504, 0.3956584930419922, 1.8098184626294533e-12, 0.5492562651634216, 0.31642255187034607, 0.1705709993839264, 9.605312101257368e-09]
[2025-06-12 13:18:52,426]: Mean: 0.18614456
[2025-06-12 13:18:52,426]: Min: -0.00038853
[2025-06-12 13:18:52,426]: Max: 0.70227927
[2025-06-12 13:18:52,426]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-12 13:18:52,432]: Sample Values (25 elements): [-4.1170148881863126e-42, -0.011322910897433758, -6.6772384781832296e-18, -4.905245274369022e-41, 0.02136116847395897, -0.014250507578253746, 4.918277350087243e-41, -4.913092545769241e-41, 0.11752036213874817, 4.929487737801841e-41, 0.0030555997509509325, -4.907347222065509e-41, -0.01508255209773779, -0.0023162965662777424, -0.021966831758618355, 4.909449169761997e-41, -0.030491363257169724, -5.480618423820792e-41, 4.910710338379889e-41, 4.909869559301294e-41, -4.950367084920281e-41, 5.988589117138538e-41, -0.027116190642118454, 0.001565226586535573, 0.023114686831831932]
[2025-06-12 13:18:52,433]: Mean: -0.00095701
[2025-06-12 13:18:52,433]: Min: -0.27778172
[2025-06-12 13:18:52,433]: Max: 0.27195439
[2025-06-12 13:18:52,433]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-06-12 13:18:52,433]: Sample Values (25 elements): [0.21962931752204895, 0.1529289186000824, 0.29034388065338135, 0.3832003176212311, 0.6658236980438232, 0.0875840112566948, 0.23225224018096924, 0.18335305154323578, 0.10729201138019562, 0.004840914160013199, 0.4027408957481384, 0.06837118417024612, 0.3931603729724884, 0.6181210875511169, 0.02213011309504509, 0.15254651010036469, 0.4854034185409546, 0.4460736811161041, 0.02280595898628235, 0.46460744738578796, 0.23102349042892456, 0.08090374618768692, 0.4180822968482971, 0.33228427171707153, 0.44834786653518677]
[2025-06-12 13:18:52,433]: Mean: 0.24837717
[2025-06-12 13:18:52,433]: Min: -0.02891485
[2025-06-12 13:18:52,434]: Max: 0.73320043
[2025-06-12 13:18:52,434]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-06-12 13:18:52,449]: Sample Values (25 elements): [-4.93789552858779e-41, -4.947564487991632e-41, -4.916035272544323e-41, -4.966762276952882e-41, -4.917296441162216e-41, 4.911831377151349e-41, 4.90566566390832e-41, -6.230873621620299e-41, -4.905805793754752e-41, 4.960036044324123e-41, -4.935373191352006e-41, 4.915054363619296e-41, -4.928787088569679e-41, -4.917856960547946e-41, 4.924443063330272e-41, -1.5595940392755344e-22, -4.946443449220172e-41, 0.005774111486971378, 4.926825270719624e-41, -4.946863838759469e-41, -4.92150033655519e-41, 4.952889422156066e-41, -4.923041764865947e-41, 4.949946695380984e-41, -4.932990983962654e-41]
[2025-06-12 13:18:52,449]: Mean: -0.00017294
[2025-06-12 13:18:52,449]: Min: -0.22353263
[2025-06-12 13:18:52,449]: Max: 0.31542403
[2025-06-12 13:18:52,449]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-06-12 13:18:52,450]: Sample Values (25 elements): [1.9821216160017765e-25, 7.136970864556752e-15, -4.928226569183949e-41, -1.1298582370838167e-28, 0.0010822222102433443, -4.9254239722553e-41, 6.7123208070647e-25, -5.17163211243717e-41, 4.913653065154971e-41, 0.6101521849632263, 4.909449169761997e-41, 1.7043419975985647e-25, -5.703705139341303e-41, 8.526143484375019e-19, 4.932990983962654e-41, 3.265421154056358e-27, 0.3425268232822418, 4.904824884829725e-41, -4.905525534061887e-41, 0.01039109192788601, 0.004589709918946028, -4.912111636844214e-41, 8.109429239233634e-10, 1.539205146882515e-19, -4.917997090394378e-41]
[2025-06-12 13:18:52,450]: Mean: 0.06327148
[2025-06-12 13:18:52,450]: Min: -0.01402698
[2025-06-12 13:18:52,450]: Max: 0.86961317
[2025-06-12 13:18:52,450]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-12 13:18:52,490]: Sample Values (25 elements): [-5.544380301296437e-18, 4.944761891062982e-41, -4.962978771099205e-41, -4.913793195001404e-41, 5.009501880114789e-41, -2.901170091718086e-06, -4.911551117458484e-41, -0.004213339649140835, -4.991425129924998e-41, 4.959755784631258e-41, -4.915334623312161e-41, -4.9254239722553e-41, -3.4481869626449294e-18, 3.537228985805996e-05, 4.904684754983292e-41, -5.838790311302215e-41, 5.869130745850271e-06, -5.004457205643219e-41, 0.006058823317289352, -4.936774489816331e-41, 4.944902020909414e-41, -5.155937569636732e-41, -4.945462540295144e-41, 5.187046395544743e-41, -4.912812286076376e-41]
[2025-06-12 13:18:52,490]: Mean: 0.00002248
[2025-06-12 13:18:52,491]: Min: -0.20098117
[2025-06-12 13:18:52,491]: Max: 0.16941774
[2025-06-12 13:18:52,491]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-06-12 13:18:52,491]: Sample Values (25 elements): [0.30890071392059326, 0.0029490499291568995, 0.46740105748176575, 0.2905818819999695, 0.21818925440311432, 0.008353585377335548, 0.4108060300350189, 2.38021435243832e-13, 0.17124301195144653, 0.24037207663059235, 0.1938244104385376, 0.45262810587882996, 0.5187429189682007, 0.012335511855781078, 0.28471654653549194, 0.3169925808906555, 1.95498023151941e-12, 0.22929009795188904, 0.41911405324935913, -0.0048713646829128265, 0.5465486645698547, 0.2874465584754944, 0.5351090431213379, 0.27350375056266785, 0.5783848762512207]
[2025-06-12 13:18:52,491]: Mean: 0.32277152
[2025-06-12 13:18:52,492]: Min: -0.01795833
[2025-06-12 13:18:52,492]: Max: 0.86113840
[2025-06-12 13:18:52,492]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-06-12 13:18:52,493]: Sample Values (25 elements): [0.0018459571292623878, -0.003940935246646404, -5.059345858171582e-05, 0.014129340648651123, 4.908047871297672e-41, -0.006086467299610376, 0.0020053123589605093, -0.042973317205905914, 0.0037388596683740616, 0.025824623182415962, 0.0069149453192949295, 0.0013459553010761738, -0.006806367076933384, -0.026510633528232574, -0.000217211723793298, -0.004111446440219879, -0.025258339941501617, 0.04240499809384346, 8.226545469369739e-05, -0.008975713513791561, 0.005425672512501478, -0.009951331652700901, -0.008645011112093925, -0.001962576759979129, 0.07485395669937134]
[2025-06-12 13:18:52,493]: Mean: -0.00017513
[2025-06-12 13:18:52,493]: Min: -0.20019549
[2025-06-12 13:18:52,493]: Max: 0.26730648
[2025-06-12 13:18:52,494]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-06-12 13:18:52,494]: Sample Values (25 elements): [0.39512354135513306, 0.44179767370224, 0.22436779737472534, 0.4433958828449249, 0.2515189051628113, 0.0546906441450119, 0.30066660046577454, 0.31568434834480286, 0.36106356978416443, 0.30204448103904724, 0.053044117987155914, 0.477164626121521, 0.47050741314888, 0.1660585254430771, 0.016169633716344833, 0.3280175030231476, 0.2691228687763214, 0.5095632672309875, 0.4347744286060333, 0.14871007204055786, 0.6864599585533142, 0.1673012673854828, 0.3443741500377655, 0.36649778485298157, 0.3712655305862427]
[2025-06-12 13:18:52,494]: Mean: 0.32011259
[2025-06-12 13:18:52,494]: Min: -0.00000000
[2025-06-12 13:18:52,494]: Max: 0.71045941
[2025-06-12 13:18:52,494]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-12 13:18:52,521]: Sample Values (25 elements): [-4.916876051622918e-41, 1.3971603739077273e-37, -4.93733500920206e-41, -4.954150590773958e-41, 4.911971506997781e-41, 4.956813057856175e-41, 4.937755398741358e-41, -1.7589843860749548e-20, -5.024776033375929e-41, -4.904684754983292e-41, -4.949526305841686e-41, -4.95709331754904e-41, -4.904684754983292e-41, -4.959055135399095e-41, -4.931029166112599e-41, 5.037107459861987e-41, -4.927666049798219e-41, 6.06888351914435e-41, 5.051400704198101e-41, -4.924583193176705e-41, 4.968583964956504e-41, 4.975590457278128e-41, -4.991845519464296e-41, -4.942099423980765e-41, -4.967322796338612e-41]
[2025-06-12 13:18:52,522]: Mean: -0.00001301
[2025-06-12 13:18:52,522]: Min: -0.07488315
[2025-06-12 13:18:52,522]: Max: 0.04957852
[2025-06-12 13:18:52,522]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-06-12 13:18:52,523]: Sample Values (25 elements): [-5.039910056790637e-41, 4.947564487991632e-41, 0.0040811155922710896, -4.917016181469351e-41, -4.91925825901227e-41, -0.006847491022199392, 4.921079947015893e-41, -5.072420181162973e-41, -0.005073856562376022, -4.918277350087243e-41, -4.917576700855081e-41, 1.0404529668384122e-11, 4.926685140873192e-41, 4.909729429454862e-41, 0.003918417729437351, -0.00495484983548522, 6.442286618448326e-14, -4.921920726094487e-41, -0.014737267047166824, 0.009153565391898155, -4.938596177819953e-41, 2.6288955008117038e-14, -0.002003377303481102, 5.017909670900737e-41, -0.0036895235534757376]
[2025-06-12 13:18:52,523]: Mean: 0.00219503
[2025-06-12 13:18:52,523]: Min: -0.06172858
[2025-06-12 13:18:52,523]: Max: 0.29880232
[2025-06-12 13:18:52,523]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-12 13:18:52,553]: Sample Values (25 elements): [5.009922269654086e-41, 4.905385404215455e-41, 4.948125007377362e-41, -4.905945923601185e-41, -4.933551503348383e-41, -4.906506442986914e-41, -4.921079947015893e-41, -6.117788835549286e-41, -4.939156697205683e-41, 5.26930261540061e-41, 4.938736307666385e-41, -4.958634745859798e-41, -4.943780982137955e-41, -4.920519427630163e-41, 4.923602284251677e-41, 5.026317461686686e-41, 4.946723708913037e-41, 4.917156311315783e-41, 4.918837869472973e-41, -4.972927990195911e-41, 5.282755080658128e-41, 0.011789248324930668, 4.944761891062982e-41, 4.944201371677252e-41, -4.939717216591413e-41]
[2025-06-12 13:18:52,553]: Mean: -0.00000691
[2025-06-12 13:18:52,553]: Min: -0.07585663
[2025-06-12 13:18:52,554]: Max: 0.07083442
[2025-06-12 13:18:52,554]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-06-12 13:18:52,554]: Sample Values (25 elements): [0.10799669474363327, 0.033721789717674255, 0.11189115047454834, 0.060591887682676315, -0.007624003104865551, 0.023315511643886566, 0.07100572437047958, 0.08610135316848755, -0.02099166251718998, -0.017543097957968712, 0.014004170894622803, 0.006832801271229982, 0.03886261582374573, 0.011760259978473186, -0.02282971888780594, -0.023916784673929214, -0.0026449894066900015, 0.09304755926132202, 0.14443908631801605, 0.014839207753539085, 5.582103543705894e-34, 0.10496077686548233, 0.044404033571481705, 0.018719887360930443, 0.0015941228484734893]
[2025-06-12 13:18:52,554]: Mean: 0.03799397
[2025-06-12 13:18:52,554]: Min: -0.03988303
[2025-06-12 13:18:52,554]: Max: 0.22896752
[2025-06-12 13:18:52,555]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-06-12 13:18:52,555]: Sample Values (25 elements): [-0.0802597776055336, -0.012334696017205715, -0.035403717309236526, 0.03821641206741333, 0.11097924411296844, -0.025762705132365227, 1.010893470265728e-06, -0.04389341175556183, -2.2962617918231842e-10, 0.008229073137044907, -0.03963043913245201, -0.07696879655122757, -1.3810105059519628e-08, 0.08135566860437393, -0.21401160955429077, 0.1667075753211975, 0.1217072606086731, -1.8578626137522036e-10, 0.07912706583738327, -0.215777188539505, -0.22308193147182465, 0.0012070407392457128, -0.07300643622875214, -0.047431815415620804, 0.09532015025615692]
[2025-06-12 13:18:52,555]: Mean: -0.02142503
[2025-06-12 13:18:52,555]: Min: -0.56413478
[2025-06-12 13:18:52,555]: Max: 0.32629502
[2025-06-12 13:18:52,555]: Checkpoint of model at path [checkpoint/ResNet18_parametrized_relu.ckpt] will be used for QAT
[2025-06-12 13:18:52,555]: 


QAT of ResNet18 with parametrized_relu down to 4 bits...
[2025-06-12 13:18:52,758]: [ResNet18_parametrized_relu_quantized_4_bits] after configure_qat:
[2025-06-12 13:18:52,841]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-06-12 13:20:59,290]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 001 Train Loss: 0.1951 Train Acc: 0.9325 Eval Loss: 0.3269 Eval Acc: 0.9018 (LR: 0.00100000)
[2025-06-12 13:23:05,754]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 002 Train Loss: 0.2032 Train Acc: 0.9291 Eval Loss: 0.3305 Eval Acc: 0.9011 (LR: 0.00100000)
[2025-06-12 13:25:12,188]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 003 Train Loss: 0.2118 Train Acc: 0.9256 Eval Loss: 0.3361 Eval Acc: 0.8960 (LR: 0.00100000)
[2025-06-12 13:27:18,410]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 004 Train Loss: 0.2012 Train Acc: 0.9299 Eval Loss: 0.3359 Eval Acc: 0.9015 (LR: 0.00100000)
[2025-06-12 13:29:24,703]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 005 Train Loss: 0.2015 Train Acc: 0.9299 Eval Loss: 0.3088 Eval Acc: 0.9062 (LR: 0.00100000)
[2025-06-12 13:31:31,060]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 006 Train Loss: 0.2062 Train Acc: 0.9289 Eval Loss: 0.3382 Eval Acc: 0.8986 (LR: 0.00100000)
[2025-06-12 13:33:37,396]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 007 Train Loss: 0.2025 Train Acc: 0.9290 Eval Loss: 0.3247 Eval Acc: 0.8948 (LR: 0.00100000)
[2025-06-12 13:35:43,639]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 008 Train Loss: 0.2004 Train Acc: 0.9295 Eval Loss: 0.3516 Eval Acc: 0.8946 (LR: 0.00100000)
[2025-06-12 13:37:49,807]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 009 Train Loss: 0.1996 Train Acc: 0.9308 Eval Loss: 0.3416 Eval Acc: 0.8957 (LR: 0.00100000)
[2025-06-12 13:39:55,857]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 010 Train Loss: 0.1987 Train Acc: 0.9312 Eval Loss: 0.3650 Eval Acc: 0.8888 (LR: 0.00100000)
[2025-06-12 13:42:01,732]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 011 Train Loss: 0.1924 Train Acc: 0.9328 Eval Loss: 0.3488 Eval Acc: 0.8918 (LR: 0.00010000)
[2025-06-12 13:44:08,180]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 012 Train Loss: 0.1277 Train Acc: 0.9562 Eval Loss: 0.2418 Eval Acc: 0.9274 (LR: 0.00010000)
[2025-06-12 13:46:14,590]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 013 Train Loss: 0.0982 Train Acc: 0.9667 Eval Loss: 0.2400 Eval Acc: 0.9292 (LR: 0.00010000)
[2025-06-12 13:48:21,057]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 014 Train Loss: 0.0884 Train Acc: 0.9699 Eval Loss: 0.2405 Eval Acc: 0.9318 (LR: 0.00010000)
[2025-06-12 13:50:27,497]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 015 Train Loss: 0.0812 Train Acc: 0.9727 Eval Loss: 0.2495 Eval Acc: 0.9309 (LR: 0.00010000)
[2025-06-12 13:52:33,788]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 016 Train Loss: 0.0764 Train Acc: 0.9739 Eval Loss: 0.2463 Eval Acc: 0.9295 (LR: 0.00010000)
[2025-06-12 13:54:40,380]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 017 Train Loss: 0.0722 Train Acc: 0.9753 Eval Loss: 0.2521 Eval Acc: 0.9308 (LR: 0.00010000)
[2025-06-12 13:56:46,559]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 018 Train Loss: 0.0688 Train Acc: 0.9764 Eval Loss: 0.2513 Eval Acc: 0.9317 (LR: 0.00010000)
[2025-06-12 13:58:52,818]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 019 Train Loss: 0.0657 Train Acc: 0.9773 Eval Loss: 0.2539 Eval Acc: 0.9337 (LR: 0.00001000)
[2025-06-12 14:00:59,965]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 020 Train Loss: 0.0579 Train Acc: 0.9806 Eval Loss: 0.2474 Eval Acc: 0.9329 (LR: 0.00001000)
[2025-06-12 14:03:06,594]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 021 Train Loss: 0.0556 Train Acc: 0.9814 Eval Loss: 0.2464 Eval Acc: 0.9332 (LR: 0.00001000)
[2025-06-12 14:05:12,945]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 022 Train Loss: 0.0527 Train Acc: 0.9820 Eval Loss: 0.2470 Eval Acc: 0.9327 (LR: 0.00001000)
[2025-06-12 14:07:19,313]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 023 Train Loss: 0.0537 Train Acc: 0.9824 Eval Loss: 0.2457 Eval Acc: 0.9338 (LR: 0.00001000)
[2025-06-12 14:09:25,787]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 024 Train Loss: 0.0512 Train Acc: 0.9828 Eval Loss: 0.2464 Eval Acc: 0.9328 (LR: 0.00001000)
[2025-06-12 14:11:31,766]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 025 Train Loss: 0.0505 Train Acc: 0.9836 Eval Loss: 0.2513 Eval Acc: 0.9334 (LR: 0.00000100)
[2025-06-12 14:13:38,215]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 026 Train Loss: 0.0495 Train Acc: 0.9835 Eval Loss: 0.2498 Eval Acc: 0.9338 (LR: 0.00000100)
[2025-06-12 14:15:44,702]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 027 Train Loss: 0.0519 Train Acc: 0.9829 Eval Loss: 0.2475 Eval Acc: 0.9331 (LR: 0.00000100)
[2025-06-12 14:17:51,143]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 028 Train Loss: 0.0494 Train Acc: 0.9837 Eval Loss: 0.2494 Eval Acc: 0.9329 (LR: 0.00000100)
[2025-06-12 14:19:57,352]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 029 Train Loss: 0.0525 Train Acc: 0.9826 Eval Loss: 0.2464 Eval Acc: 0.9338 (LR: 0.00000100)
[2025-06-12 14:22:03,852]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 030 Train Loss: 0.0515 Train Acc: 0.9826 Eval Loss: 0.2480 Eval Acc: 0.9342 (LR: 0.00000100)
[2025-06-12 14:24:10,636]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 031 Train Loss: 0.0492 Train Acc: 0.9839 Eval Loss: 0.2459 Eval Acc: 0.9349 (LR: 0.00000010)
[2025-06-12 14:26:16,845]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 032 Train Loss: 0.0523 Train Acc: 0.9822 Eval Loss: 0.2495 Eval Acc: 0.9321 (LR: 0.00000010)
[2025-06-12 14:28:22,979]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 033 Train Loss: 0.0510 Train Acc: 0.9832 Eval Loss: 0.2490 Eval Acc: 0.9322 (LR: 0.00000010)
[2025-06-12 14:30:29,142]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 034 Train Loss: 0.0502 Train Acc: 0.9833 Eval Loss: 0.2488 Eval Acc: 0.9330 (LR: 0.00000010)
[2025-06-12 14:32:35,272]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 035 Train Loss: 0.0503 Train Acc: 0.9821 Eval Loss: 0.2479 Eval Acc: 0.9335 (LR: 0.00000010)
[2025-06-12 14:34:41,605]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 036 Train Loss: 0.0500 Train Acc: 0.9837 Eval Loss: 0.2514 Eval Acc: 0.9329 (LR: 0.00000010)
[2025-06-12 14:36:47,667]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 037 Train Loss: 0.0493 Train Acc: 0.9839 Eval Loss: 0.2506 Eval Acc: 0.9336 (LR: 0.00000010)
[2025-06-12 14:38:53,742]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 038 Train Loss: 0.0507 Train Acc: 0.9835 Eval Loss: 0.2468 Eval Acc: 0.9339 (LR: 0.00000010)
[2025-06-12 14:40:59,950]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 039 Train Loss: 0.0490 Train Acc: 0.9841 Eval Loss: 0.2517 Eval Acc: 0.9325 (LR: 0.00000010)
[2025-06-12 14:43:06,994]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 040 Train Loss: 0.0489 Train Acc: 0.9839 Eval Loss: 0.2472 Eval Acc: 0.9326 (LR: 0.00000010)
[2025-06-12 14:45:13,042]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 041 Train Loss: 0.0489 Train Acc: 0.9839 Eval Loss: 0.2496 Eval Acc: 0.9330 (LR: 0.00000010)
[2025-06-12 14:47:19,117]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 042 Train Loss: 0.0492 Train Acc: 0.9838 Eval Loss: 0.2502 Eval Acc: 0.9326 (LR: 0.00000010)
[2025-06-12 14:49:24,959]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 043 Train Loss: 0.0509 Train Acc: 0.9826 Eval Loss: 0.2477 Eval Acc: 0.9341 (LR: 0.00000010)
[2025-06-12 14:51:30,959]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 044 Train Loss: 0.0505 Train Acc: 0.9833 Eval Loss: 0.2482 Eval Acc: 0.9332 (LR: 0.00000010)
[2025-06-12 14:53:36,875]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 045 Train Loss: 0.0519 Train Acc: 0.9830 Eval Loss: 0.2492 Eval Acc: 0.9335 (LR: 0.00000010)
[2025-06-12 14:55:42,718]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 046 Train Loss: 0.0493 Train Acc: 0.9841 Eval Loss: 0.2459 Eval Acc: 0.9342 (LR: 0.00000010)
[2025-06-12 14:57:48,703]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 047 Train Loss: 0.0500 Train Acc: 0.9832 Eval Loss: 0.2477 Eval Acc: 0.9337 (LR: 0.00000010)
[2025-06-12 14:59:54,977]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 048 Train Loss: 0.0492 Train Acc: 0.9837 Eval Loss: 0.2463 Eval Acc: 0.9327 (LR: 0.00000010)
[2025-06-12 15:02:00,799]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 049 Train Loss: 0.0491 Train Acc: 0.9844 Eval Loss: 0.2478 Eval Acc: 0.9334 (LR: 0.00000010)
[2025-06-12 15:04:06,735]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 050 Train Loss: 0.0503 Train Acc: 0.9841 Eval Loss: 0.2498 Eval Acc: 0.9340 (LR: 0.00000010)
[2025-06-12 15:06:12,676]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 051 Train Loss: 0.0499 Train Acc: 0.9830 Eval Loss: 0.2487 Eval Acc: 0.9345 (LR: 0.00000010)
[2025-06-12 15:08:17,924]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 052 Train Loss: 0.0496 Train Acc: 0.9840 Eval Loss: 0.2465 Eval Acc: 0.9334 (LR: 0.00000010)
[2025-06-12 15:10:06,039]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 053 Train Loss: 0.0510 Train Acc: 0.9833 Eval Loss: 0.2506 Eval Acc: 0.9330 (LR: 0.00000010)
[2025-06-12 15:11:57,129]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 054 Train Loss: 0.0513 Train Acc: 0.9824 Eval Loss: 0.2494 Eval Acc: 0.9338 (LR: 0.00000010)
[2025-06-12 15:13:46,952]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 055 Train Loss: 0.0495 Train Acc: 0.9839 Eval Loss: 0.2475 Eval Acc: 0.9338 (LR: 0.00000010)
[2025-06-12 15:16:04,623]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 056 Train Loss: 0.0505 Train Acc: 0.9828 Eval Loss: 0.2495 Eval Acc: 0.9342 (LR: 0.00000010)
[2025-06-12 15:18:20,311]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 057 Train Loss: 0.0510 Train Acc: 0.9828 Eval Loss: 0.2486 Eval Acc: 0.9333 (LR: 0.00000010)
[2025-06-12 15:20:27,240]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 058 Train Loss: 0.0496 Train Acc: 0.9838 Eval Loss: 0.2481 Eval Acc: 0.9334 (LR: 0.00000010)
[2025-06-12 15:22:34,092]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 059 Train Loss: 0.0506 Train Acc: 0.9828 Eval Loss: 0.2520 Eval Acc: 0.9330 (LR: 0.00000010)
[2025-06-12 15:24:41,443]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 060 Train Loss: 0.0501 Train Acc: 0.9835 Eval Loss: 0.2481 Eval Acc: 0.9332 (LR: 0.00000010)
[2025-06-12 15:24:41,444]: [ResNet18_parametrized_relu_quantized_4_bits] Best Eval Accuracy: 0.9349
[2025-06-12 15:24:41,539]: 


Quantization of model down to 4 bits finished
[2025-06-12 15:24:41,539]: Model Architecture:
[2025-06-12 15:24:41,595]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1498], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.2475433349609375)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0572], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.42667722702026367, max_val=0.43059033155441284)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1196], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0574], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3937631845474243, max_val=0.4673522114753723)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1936], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0595], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4175482392311096, max_val=0.47448933124542236)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1132], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0445], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3425453305244446, max_val=0.3253146708011627)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2111], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0514], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.43272268772125244, max_val=0.3381630778312683)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1398], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0431], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32961922883987427, max_val=0.3167491555213928)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0609], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.40680044889450073, max_val=0.5067688226699829)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1857], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0400], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30129921436309814, max_val=0.29942774772644043)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1294], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0366], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24723362922668457, max_val=0.30243152379989624)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1926], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0380], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24469634890556335, max_val=0.32590293884277344)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1370], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0425], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25199252367019653, max_val=0.3851022720336914)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0486], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3159964084625244, max_val=0.4125422239303589)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1579], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0401], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27763232588768005, max_val=0.3239535689353943)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1137], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0398], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30928561091423035, max_val=0.2873739004135132)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1623], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0362], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23329457640647888, max_val=0.3089744448661804)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1526], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0237], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17175669968128204, max_val=0.1832607090473175)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0280], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17316865921020508, max_val=0.24619299173355103)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1653], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0037], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02088729850947857, max_val=0.035055600106716156)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0176], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0053], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04466162621974945, max_val=0.03513314574956894)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1653], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-06-12 15:24:41,595]: 
Model Weights:
[2025-06-12 15:24:41,595]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-06-12 15:24:41,596]: Sample Values (25 elements): [-0.024339256808161736, 0.15137344598770142, -0.09988505393266678, 0.044176798313856125, -0.026772433891892433, -0.10182230919599533, 0.02198391780257225, 0.02878786437213421, 0.06025004759430885, -0.12443533539772034, -0.045179545879364014, 0.06531166285276413, 0.2636324465274811, -0.2747873067855835, 0.10736358910799026, -0.08685878664255142, -0.1935206651687622, -0.04913271591067314, -0.12414810061454773, -0.08290496468544006, -0.11265061795711517, -0.20088407397270203, -0.058174435049295425, -0.07045163959264755, 0.00926938746124506]
[2025-06-12 15:24:41,596]: Mean: -0.00210838
[2025-06-12 15:24:41,596]: Min: -0.44239706
[2025-06-12 15:24:41,596]: Max: 0.41392821
[2025-06-12 15:24:41,596]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-06-12 15:24:41,596]: Sample Values (25 elements): [0.4793583154678345, 0.5085591673851013, 0.9082841873168945, 0.5224809050559998, 0.7185572385787964, 0.5204117894172668, 0.16527217626571655, 0.6794608235359192, 0.9506248831748962, 0.4837532043457031, 0.9364519119262695, 0.6881848573684692, 1.246492862701416, 0.4875258207321167, 0.8163841962814331, 0.20667868852615356, 0.48795241117477417, 0.788749635219574, 0.7332209944725037, 0.8163734078407288, 0.3997560441493988, 0.6514723896980286, 0.7004588842391968, 0.8023216128349304, 0.31833627820014954]
[2025-06-12 15:24:41,597]: Mean: 0.64232129
[2025-06-12 15:24:41,597]: Min: 0.16527218
[2025-06-12 15:24:41,597]: Max: 1.24649286
[2025-06-12 15:24:41,598]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 15:24:41,598]: Sample Values (25 elements): [0.0, 0.0, 0.17145352065563202, -0.05715117231011391, -0.11430234462022781, 0.0, 0.0, -0.11430234462022781, 0.0, -0.05715117231011391, 0.0, 0.0, 0.0, 0.17145352065563202, 0.05715117231011391, -0.05715117231011391, 0.0, 0.0, 0.0, -0.05715117231011391, -0.11430234462022781, 0.0, 0.0, -0.05715117231011391, 0.0]
[2025-06-12 15:24:41,599]: Mean: -0.00826788
[2025-06-12 15:24:41,599]: Min: -0.40005821
[2025-06-12 15:24:41,599]: Max: 0.45720938
[2025-06-12 15:24:41,599]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-06-12 15:24:41,599]: Sample Values (25 elements): [0.48547956347465515, 0.4699191749095917, 0.5891833305358887, 0.6624162197113037, 0.5910589098930359, 0.602541983127594, 0.6864839792251587, 0.7320261001586914, 0.7325555086135864, 0.698443591594696, 0.414808452129364, 0.46716344356536865, 0.6179667711257935, 0.559641420841217, 0.5804717540740967, 0.7540687918663025, 0.9188876152038574, 0.5797119736671448, 0.6836869120597839, 0.6600614190101624, 0.5906786918640137, 0.6530027985572815, 0.6831762790679932, 0.46896374225616455, 0.5965158343315125]
[2025-06-12 15:24:41,599]: Mean: 0.63034987
[2025-06-12 15:24:41,600]: Min: 0.29860505
[2025-06-12 15:24:41,600]: Max: 1.02030885
[2025-06-12 15:24:41,602]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 15:24:41,602]: Sample Values (25 elements): [0.0, -0.057407692074775696, 0.0, 0.0, 0.057407692074775696, 0.057407692074775696, -0.057407692074775696, -0.057407692074775696, 0.0, -0.057407692074775696, 0.0, 0.057407692074775696, 0.057407692074775696, 0.0, -0.057407692074775696, 0.0, 0.0, 0.0, 0.057407692074775696, -0.057407692074775696, 0.057407692074775696, 0.0, 0.0, 0.0, 0.057407692074775696]
[2025-06-12 15:24:41,602]: Mean: -0.00215372
[2025-06-12 15:24:41,603]: Min: -0.40185386
[2025-06-12 15:24:41,603]: Max: 0.45926154
[2025-06-12 15:24:41,603]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-06-12 15:24:41,603]: Sample Values (25 elements): [0.5420992374420166, 0.49788352847099304, 0.5897356271743774, 0.7757283449172974, 0.5003184676170349, 0.9680471420288086, 0.8537445068359375, 0.673215389251709, 0.7656289935112, 1.0685300827026367, 0.6238226294517517, 0.7046763300895691, 0.809770405292511, 0.6420509219169617, 0.3847103714942932, 0.5033117532730103, 0.6812576651573181, 1.2278648614883423, 0.588603675365448, 1.2048897743225098, 0.5635859370231628, 0.78049635887146, 0.7558410167694092, 1.0399670600891113, 0.5655459761619568]
[2025-06-12 15:24:41,603]: Mean: 0.71594113
[2025-06-12 15:24:41,603]: Min: 0.27197805
[2025-06-12 15:24:41,604]: Max: 1.28437388
[2025-06-12 15:24:41,604]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 15:24:41,605]: Sample Values (25 elements): [-0.05946917459368706, 0.05946917459368706, 0.17840752005577087, 0.05946917459368706, 0.0, 0.0, 0.0, -0.05946917459368706, 0.0, 0.05946917459368706, 0.05946917459368706, 0.11893834918737411, 0.05946917459368706, 0.0, 0.0, 0.05946917459368706, 0.0, 0.11893834918737411, 0.0, -0.05946917459368706, 0.0, -0.05946917459368706, 0.0, 0.0, -0.05946917459368706]
[2025-06-12 15:24:41,605]: Mean: -0.00501868
[2025-06-12 15:24:41,605]: Min: -0.41628423
[2025-06-12 15:24:41,606]: Max: 0.47575340
[2025-06-12 15:24:41,606]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-06-12 15:24:41,606]: Sample Values (25 elements): [0.8248307108879089, 0.47112420201301575, 0.4771444797515869, 0.4749002754688263, 0.6660372018814087, 0.4852840006351471, 0.4143398702144623, 0.6165360808372498, 0.5447728633880615, 0.6902624368667603, 0.498921275138855, 0.6944340467453003, 0.6604474782943726, 0.5249311923980713, 0.5514512658119202, 0.679813802242279, 0.9944238066673279, 1.0878329277038574, 0.6494154930114746, 0.5716348886489868, 0.5394118428230286, 0.4409385323524475, 0.6751384735107422, 0.5491047501564026, 0.5972694754600525]
[2025-06-12 15:24:41,606]: Mean: 0.57217062
[2025-06-12 15:24:41,606]: Min: 0.07658237
[2025-06-12 15:24:41,606]: Max: 1.08783293
[2025-06-12 15:24:41,608]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 15:24:41,609]: Sample Values (25 elements): [0.0, -0.04452400282025337, 0.0, 0.0, 0.04452400282025337, -0.08904800564050674, 0.04452400282025337, -0.04452400282025337, 0.0, -0.04452400282025337, 0.04452400282025337, 0.0, 0.0, 0.08904800564050674, -0.08904800564050674, 0.0, 0.0, 0.0, 0.08904800564050674, 0.13357201218605042, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 15:24:41,609]: Mean: -0.00282865
[2025-06-12 15:24:41,609]: Min: -0.35619202
[2025-06-12 15:24:41,609]: Max: 0.31166801
[2025-06-12 15:24:41,609]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-06-12 15:24:41,609]: Sample Values (25 elements): [0.40463292598724365, 0.4361940920352936, 0.6673581600189209, 0.650444507598877, 1.0022748708724976, 0.6385710835456848, 0.7542140483856201, 0.4813844561576843, 0.45735689997673035, 0.436160147190094, 0.6382530927658081, 0.6238555312156677, 0.680251955986023, 0.6349133849143982, 0.8331297636032104, 0.5457460880279541, 0.7485504150390625, 0.4818931818008423, 0.8976847529411316, 0.5433303117752075, 0.6038186550140381, 0.6709245443344116, 0.7950672507286072, 0.5769733190536499, 0.5076919198036194]
[2025-06-12 15:24:41,610]: Mean: 0.63356262
[2025-06-12 15:24:41,610]: Min: 0.20366393
[2025-06-12 15:24:41,610]: Max: 1.00227487
[2025-06-12 15:24:41,611]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-06-12 15:24:41,612]: Sample Values (25 elements): [0.051392387598752975, -0.051392387598752975, 0.0, 0.0, 0.0, 0.051392387598752975, -0.051392387598752975, -0.051392387598752975, 0.0, 0.0, 0.10278477519750595, -0.051392387598752975, 0.0, 0.0, 0.0, 0.10278477519750595, -0.10278477519750595, 0.051392387598752975, -0.10278477519750595, 0.0, 0.0, 0.051392387598752975, 0.0, 0.0, 0.0]
[2025-06-12 15:24:41,612]: Mean: -0.00220827
[2025-06-12 15:24:41,612]: Min: -0.41113910
[2025-06-12 15:24:41,612]: Max: 0.35974672
[2025-06-12 15:24:41,612]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-06-12 15:24:41,613]: Sample Values (25 elements): [0.6647852659225464, 0.561020016670227, 0.5001448392868042, 0.6528070569038391, 0.3934531509876251, 0.5763728022575378, 0.7824136018753052, 0.7289470434188843, 0.5244778990745544, 0.6193875074386597, 0.38911959528923035, 0.46059009432792664, 0.5004595518112183, 0.3662528991699219, 0.4371779263019562, 0.36717599630355835, 0.2798924744129181, 0.6148765683174133, 0.5056411027908325, 0.4809402823448181, 0.6192257404327393, 0.7770897746086121, 0.6208943128585815, 0.27193447947502136, 0.6977470517158508]
[2025-06-12 15:24:41,613]: Mean: 0.47826338
[2025-06-12 15:24:41,613]: Min: -0.00000000
[2025-06-12 15:24:41,614]: Max: 0.91780472
[2025-06-12 15:24:41,615]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-12 15:24:41,616]: Sample Values (25 elements): [0.04309122636914253, 0.0, 0.0, -0.04309122636914253, 0.04309122636914253, -0.04309122636914253, -0.04309122636914253, -0.04309122636914253, -0.04309122636914253, 0.04309122636914253, 0.04309122636914253, -0.08618245273828506, 0.0, -0.04309122636914253, -0.04309122636914253, 0.0, -0.08618245273828506, 0.04309122636914253, -0.04309122636914253, 0.0, 0.04309122636914253, 0.04309122636914253, 0.1292736828327179, 0.08618245273828506, 0.04309122636914253]
[2025-06-12 15:24:41,617]: Mean: -0.00269086
[2025-06-12 15:24:41,617]: Min: -0.34472981
[2025-06-12 15:24:41,617]: Max: 0.30163857
[2025-06-12 15:24:41,617]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-06-12 15:24:41,617]: Sample Values (25 elements): [0.747754693031311, 0.748172402381897, 0.7699368000030518, 0.48511505126953125, 0.6247716546058655, 0.6064786314964294, 0.6004307866096497, 0.48173797130584717, 0.5607641339302063, 0.618492841720581, 0.6997007131576538, 0.678479015827179, 0.6895485520362854, 0.5415963530540466, 0.45458757877349854, 0.5780353546142578, 0.6779526472091675, 0.579567015171051, 0.5681867599487305, 0.5225979685783386, 0.6700940728187561, 0.5179271101951599, 0.5556449294090271, 0.5247016549110413, 0.6473450064659119]
[2025-06-12 15:24:41,618]: Mean: 0.60344917
[2025-06-12 15:24:41,618]: Min: 0.01528190
[2025-06-12 15:24:41,618]: Max: 0.94612634
[2025-06-12 15:24:41,619]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-06-12 15:24:41,619]: Sample Values (25 elements): [0.12180924415588379, 0.18271386623382568, 0.060904622077941895, 0.0, -0.060904622077941895, -0.18271386623382568, -0.12180924415588379, 0.0, -0.12180924415588379, 0.0, 0.060904622077941895, -0.060904622077941895, 0.12180924415588379, -0.060904622077941895, 0.0, -0.060904622077941895, 0.060904622077941895, -0.060904622077941895, 0.0, 0.060904622077941895, 0.0, -0.060904622077941895, 0.060904622077941895, 0.12180924415588379, 0.0]
[2025-06-12 15:24:41,620]: Mean: -0.00349428
[2025-06-12 15:24:41,620]: Min: -0.42633235
[2025-06-12 15:24:41,621]: Max: 0.48723698
[2025-06-12 15:24:41,621]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-06-12 15:24:41,621]: Sample Values (25 elements): [0.45339176058769226, 0.36675554513931274, 0.5371946692466736, 0.5778113603591919, 0.6815215945243835, 0.504443883895874, 0.6392128467559814, 0.5543624758720398, 0.46999046206474304, 0.4877418577671051, 0.4763953387737274, 0.3564193844795227, 0.4193911850452423, 0.3691474497318268, 0.662702202796936, 0.4855772852897644, 0.4715040326118469, 0.4914388656616211, 0.4224368929862976, 0.4906482696533203, 0.3763330578804016, 0.3307291567325592, 0.39392200112342834, 0.3990594148635864, 0.4940475523471832]
[2025-06-12 15:24:41,621]: Mean: 0.46439838
[2025-06-12 15:24:41,621]: Min: 0.01803866
[2025-06-12 15:24:41,621]: Max: 0.76633912
[2025-06-12 15:24:41,623]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-12 15:24:41,624]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.08009693026542664, 0.08009693026542664, -0.04004846513271332, 0.04004846513271332, 0.0, 0.0, 0.08009693026542664, -0.04004846513271332, 0.08009693026542664, -0.08009693026542664, 0.04004846513271332, 0.0, 0.12014539539813995, 0.04004846513271332, -0.08009693026542664, 0.12014539539813995, 0.0, 0.0, 0.0, -0.08009693026542664, -0.04004846513271332]
[2025-06-12 15:24:41,624]: Mean: -0.00356769
[2025-06-12 15:24:41,625]: Min: -0.32038772
[2025-06-12 15:24:41,625]: Max: 0.28033924
[2025-06-12 15:24:41,625]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-06-12 15:24:41,625]: Sample Values (25 elements): [0.3272559642791748, 4.403848379297415e-06, 0.4668411612510681, 0.5942894816398621, 0.024400930851697922, 0.440364271402359, 0.6086888909339905, 0.296023964881897, 0.5274308323860168, 0.6240664720535278, 0.5656866431236267, 0.49936607480049133, 0.5630277395248413, 0.553263247013092, 0.5389101505279541, 0.5951350927352905, 0.601367175579071, 0.2615942656993866, 0.581069827079773, 0.1225622296333313, 0.48066821694374084, 0.4229073226451874, 0.399809330701828, 0.4774492681026459, 0.6300100684165955]
[2025-06-12 15:24:41,625]: Mean: 0.43715286
[2025-06-12 15:24:41,625]: Min: -0.00000000
[2025-06-12 15:24:41,626]: Max: 0.77208674
[2025-06-12 15:24:41,627]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-12 15:24:41,628]: Sample Values (25 elements): [0.0, 0.0, -0.073288694024086, 0.073288694024086, 0.036644347012043, 0.0, 0.036644347012043, 0.073288694024086, 0.0, -0.036644347012043, -0.036644347012043, 0.036644347012043, 0.0, 0.0, -0.073288694024086, -0.036644347012043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.146577388048172, -0.073288694024086, -0.073288694024086, 0.0]
[2025-06-12 15:24:41,629]: Mean: -0.00279226
[2025-06-12 15:24:41,629]: Min: -0.25651044
[2025-06-12 15:24:41,629]: Max: 0.29315478
[2025-06-12 15:24:41,629]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-06-12 15:24:41,629]: Sample Values (25 elements): [0.35572952032089233, 0.49577993154525757, 0.53794926404953, 0.4832521378993988, 0.4506256580352783, 0.5459323525428772, 0.2879311442375183, 0.014713404700160027, 0.4139186441898346, 0.37275049090385437, 0.2502862215042114, 0.3609298765659332, 0.47268086671829224, 0.5763880014419556, 0.5329116582870483, 0.5587818026542664, 0.3166530728340149, 0.6212418675422668, 0.5271881818771362, 0.33335599303245544, 0.3367714285850525, 0.499507337808609, 0.38705864548683167, 0.47399765253067017, 0.39672335982322693]
[2025-06-12 15:24:41,630]: Mean: 0.44950181
[2025-06-12 15:24:41,630]: Min: 0.00009582
[2025-06-12 15:24:41,630]: Max: 0.77869409
[2025-06-12 15:24:41,631]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-06-12 15:24:41,634]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15215982496738434, -0.07607991248369217, -0.038039956241846085, 0.038039956241846085, -0.038039956241846085, 0.0, -0.07607991248369217, 0.07607991248369217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07607991248369217, 0.0, 0.0, 0.0]
[2025-06-12 15:24:41,634]: Mean: -0.00104351
[2025-06-12 15:24:41,634]: Min: -0.22823974
[2025-06-12 15:24:41,634]: Max: 0.34235960
[2025-06-12 15:24:41,634]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-06-12 15:24:41,635]: Sample Values (25 elements): [0.005908770486712456, 0.5259838700294495, 0.5322871208190918, 5.768865517932407e-41, 2.48746445663528e-08, 0.000600387342274189, 0.34349319338798523, 0.4453481435775757, 0.02592470310628414, 0.48652735352516174, 0.5170348286628723, 0.5243104696273804, 0.45970824360847473, 1.4096659469942097e-05, 0.4731021225452423, -4.909729429454862e-41, 0.49654722213745117, 0.6949740648269653, 0.5066429376602173, 4.905245274369022e-41, 0.1676659733057022, 6.271931666625016e-41, 0.5334681868553162, 0.3931007385253906, 0.5235503315925598]
[2025-06-12 15:24:41,635]: Mean: 0.28787869
[2025-06-12 15:24:41,636]: Min: -0.00000000
[2025-06-12 15:24:41,636]: Max: 0.75317621
[2025-06-12 15:24:41,637]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-12 15:24:41,643]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04247298836708069, 0.0, 0.0, 0.0, 0.0, -0.08494597673416138, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04247298836708069, 0.0, 0.08494597673416138, -0.04247298836708069, 0.0]
[2025-06-12 15:24:41,643]: Mean: -0.00089861
[2025-06-12 15:24:41,644]: Min: -0.25483793
[2025-06-12 15:24:41,644]: Max: 0.38225690
[2025-06-12 15:24:41,644]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-06-12 15:24:41,644]: Sample Values (25 elements): [0.3350456655025482, 0.0010658606188371778, 0.4343661069869995, 0.5821202993392944, 6.03216949937904e-41, 0.45250365138053894, 0.5570523142814636, 5.636863202593009e-41, 0.6394482254981995, 0.5224466919898987, 0.001180006773211062, 0.5974314212799072, -5.679322546062051e-41, 0.35016393661499023, 0.4832126498222351, 0.34214338660240173, 0.5715976357460022, 0.603198766708374, 0.5280775427818298, 0.6543818116188049, 0.5107772946357727, 0.5466232299804688, -5.678481766983456e-41, 0.46456289291381836, -5.292003650522672e-41]
[2025-06-12 15:24:41,644]: Mean: 0.39389935
[2025-06-12 15:24:41,645]: Min: -0.01145438
[2025-06-12 15:24:41,645]: Max: 0.85434604
[2025-06-12 15:24:41,646]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-06-12 15:24:41,646]: Sample Values (25 elements): [0.0, 0.09713848680257797, -0.09713848680257797, 0.048569243401288986, -0.048569243401288986, -0.048569243401288986, 0.09713848680257797, 0.048569243401288986, 0.0, 0.0, 0.0, -0.14570772647857666, -0.048569243401288986, 0.048569243401288986, -0.048569243401288986, 0.0, -0.048569243401288986, 0.0, 0.0, -0.048569243401288986, 0.14570772647857666, -0.09713848680257797, 0.048569243401288986, -0.048569243401288986, 0.048569243401288986]
[2025-06-12 15:24:41,646]: Mean: -0.00416799
[2025-06-12 15:24:41,647]: Min: -0.33998471
[2025-06-12 15:24:41,647]: Max: 0.38855395
[2025-06-12 15:24:41,647]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-06-12 15:24:41,647]: Sample Values (25 elements): [0.3236573040485382, 6.013672359649952e-41, 0.19286149740219116, 0.38145676255226135, 0.25638312101364136, 0.27690455317497253, 0.31885552406311035, 0.2961216866970062, 0.26759377121925354, 0.18034665286540985, 0.06958861649036407, 0.37751466035842896, 0.4081224203109741, 0.45678845047950745, 0.3633084297180176, 0.3950202763080597, 0.3569115698337555, 0.3221539855003357, 0.30290699005126953, 0.004572214558720589, 0.21663950383663177, 5.260894824614661e-41, 0.3146738111972809, 0.21457841992378235, 5.704265658727033e-41]
[2025-06-12 15:24:41,648]: Mean: 0.23170669
[2025-06-12 15:24:41,648]: Min: -0.00747263
[2025-06-12 15:24:41,649]: Max: 0.50533020
[2025-06-12 15:24:41,650]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-12 15:24:41,656]: Sample Values (25 elements): [0.0, 0.0, -0.040105726569890976, 0.0, 0.0, -0.040105726569890976, 0.0, 0.0, 0.12031717598438263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.040105726569890976, 0.0, 0.0]
[2025-06-12 15:24:41,656]: Mean: -0.00098227
[2025-06-12 15:24:41,656]: Min: -0.28074008
[2025-06-12 15:24:41,657]: Max: 0.32084581
[2025-06-12 15:24:41,657]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-06-12 15:24:41,657]: Sample Values (25 elements): [-5.683946830994323e-41, 5.525600104525619e-41, -4.969705003727964e-41, -4.945602670141577e-41, 0.42392510175704956, -4.950367084920281e-41, -4.909028780222699e-41, 0.352904736995697, 0.6099341511726379, -5.258092227686011e-41, 5.912638740372133e-41, -4.912251766690646e-41, 5.349456887559989e-41, -6.096769358584414e-41, 0.5768031477928162, 0.2546449303627014, 0.4580247402191162, 5.135058222518292e-41, 0.4569322168827057, 6.292811013743456e-41, -5.039769926944205e-41, 0.10902126878499985, 0.25107017159461975, -5.163084191804788e-41, 0.11344080418348312]
[2025-06-12 15:24:41,657]: Mean: 0.16261163
[2025-06-12 15:24:41,657]: Min: -0.00000000
[2025-06-12 15:24:41,658]: Max: 0.68282664
[2025-06-12 15:24:41,659]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-12 15:24:41,664]: Sample Values (25 elements): [0.0, 0.0, -0.039777304977178574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 15:24:41,665]: Mean: -0.00077636
[2025-06-12 15:24:41,665]: Min: -0.31821844
[2025-06-12 15:24:41,665]: Max: 0.27844113
[2025-06-12 15:24:41,665]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-06-12 15:24:41,665]: Sample Values (25 elements): [4.910149818994159e-41, 0.6420193314552307, 0.21605606377124786, -5.067936026077133e-41, 0.6030118465423584, 0.055946171283721924, 0.31456926465034485, 0.1815205067396164, 0.030287092551589012, 0.2573527693748474, 0.04903954267501831, 0.3766762912273407, 0.2771822512149811, 0.17554554343223572, 0.06661605834960938, 0.4275914132595062, 0.6585548520088196, 0.09799391776323318, 0.3110624849796295, 0.1361200511455536, 0.10508901625871658, 0.007695185486227274, 0.2890297472476959, 0.132272407412529, 0.37007588148117065]
[2025-06-12 15:24:41,666]: Mean: 0.20767874
[2025-06-12 15:24:41,666]: Min: -0.03602288
[2025-06-12 15:24:41,666]: Max: 0.69742322
[2025-06-12 15:24:41,667]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-06-12 15:24:41,682]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.10845380276441574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14460507035255432, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 15:24:41,682]: Mean: -0.00013187
[2025-06-12 15:24:41,683]: Min: -0.21690761
[2025-06-12 15:24:41,683]: Max: 0.32536140
[2025-06-12 15:24:41,683]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-06-12 15:24:41,684]: Sample Values (25 elements): [-4.909168910069132e-41, -4.911971506997781e-41, 5.49519192784977e-41, 4.913653065154971e-41, -4.90510514452259e-41, -4.913933324847836e-41, 0.7313418984413147, 6.149177921150162e-41, 5.498695174010582e-41, 5.213390806674049e-41, -4.943921111984387e-41, -5.970512366948748e-41, -4.98189630036759e-41, 5.905492118204077e-41, -5.731871238474232e-41, -6.0880813081056e-41, 0.6170758605003357, -5.065553818687781e-41, 5.840611999305838e-41, 4.992125779157161e-41, -4.910149818994159e-41, 0.6108182072639465, -6.144413506371458e-41, 5.550963606729898e-41, 4.981616040674725e-41]
[2025-06-12 15:24:41,684]: Mean: 0.04686963
[2025-06-12 15:24:41,684]: Min: -0.00000000
[2025-06-12 15:24:41,684]: Max: 0.84466630
[2025-06-12 15:24:41,685]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-12 15:24:41,738]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 15:24:41,738]: Mean: 0.00003728
[2025-06-12 15:24:41,739]: Min: -0.16567481
[2025-06-12 15:24:41,739]: Max: 0.18934263
[2025-06-12 15:24:41,740]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-06-12 15:24:41,741]: Sample Values (25 elements): [0.2667386531829834, 0.35498854517936707, 0.5631919503211975, 0.584930956363678, 0.1387367993593216, 0.29046711325645447, 0.3718501925468445, 0.1846710592508316, 0.5366896390914917, 0.461647629737854, -5.49911556354988e-41, 0.6541558504104614, 0.4998742341995239, 0.5665444135665894, 0.2791304588317871, 0.00022406410425901413, 0.5416345000267029, 0.5097469687461853, 5.239735217803356e-41, 1.5238941841744236e-07, 0.5737237930297852, 2.1798723537358455e-06, 0.3372156023979187, 0.2521388828754425, 0.3171718716621399]
[2025-06-12 15:24:41,741]: Mean: 0.26647925
[2025-06-12 15:24:41,741]: Min: -0.00003731
[2025-06-12 15:24:41,741]: Max: 0.85990459
[2025-06-12 15:24:41,743]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-06-12 15:24:41,744]: Sample Values (25 elements): [-0.05591489002108574, 0.0, 0.0, 0.0, 0.0, 0.0, -0.02795744501054287, 0.05591489002108574, 0.0, -0.05591489002108574, 0.0, 0.0, 0.0, 0.0, -0.05591489002108574, 0.0, -0.05591489002108574, 0.0, -0.02795744501054287, 0.0, 0.0, -0.02795744501054287, 0.0, 0.0, 0.0]
[2025-06-12 15:24:41,744]: Mean: 0.00024935
[2025-06-12 15:24:41,744]: Min: -0.16774467
[2025-06-12 15:24:41,744]: Max: 0.25161701
[2025-06-12 15:24:41,744]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-06-12 15:24:41,745]: Sample Values (25 elements): [0.33383655548095703, 0.24856533110141754, 0.363247811794281, 0.1371878832578659, 0.4140166938304901, 2.1614391580004694e-28, 0.03096356987953186, 0.3921743929386139, 0.4212861657142639, 0.4363498389720917, 0.010236130096018314, -5.97681821003821e-41, 0.008456487208604813, 0.16140210628509521, 2.153393097614753e-06, 0.35791411995887756, 0.3766138553619385, 5.833044987598484e-41, 0.290368914604187, 0.45187658071517944, 0.4616217315196991, 0.47088268399238586, 0.38898149132728577, 0.20656971633434296, 6.043379887093639e-41]
[2025-06-12 15:24:41,745]: Mean: 0.24384490
[2025-06-12 15:24:41,745]: Min: -0.00000000
[2025-06-12 15:24:41,746]: Max: 0.64817429
[2025-06-12 15:24:41,748]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-12 15:24:41,798]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 15:24:41,799]: Mean: -0.00000038
[2025-06-12 15:24:41,799]: Min: -0.02237715
[2025-06-12 15:24:41,799]: Max: 0.03356573
[2025-06-12 15:24:41,799]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-06-12 15:24:41,800]: Sample Values (25 elements): [-5.03780810909415e-41, -4.941959294134332e-41, 5.868778098438766e-41, 5.978920157734697e-41, 4.906086053447617e-41, -5.663768133108046e-41, 5.172893281055062e-41, 4.906086053447617e-41, 4.907767611604807e-41, -4.907347222065509e-41, 4.909729429454862e-41, -4.909589299608429e-41, -5.808802524165664e-41, 5.252206774135847e-41, 4.953169681848931e-41, 4.955271629545418e-41, -4.904684754983292e-41, 4.908047871297672e-41, -5.697399296251841e-41, 4.920659557476595e-41, -4.907207092219077e-41, -5.135758871750455e-41, 4.909589299608429e-41, -4.919818778398e-41, 4.911551117458484e-41]
[2025-06-12 15:24:41,800]: Mean: 0.00060868
[2025-06-12 15:24:41,800]: Min: -0.01016392
[2025-06-12 15:24:41,800]: Max: 0.17776920
[2025-06-12 15:24:41,802]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-12 15:24:41,851]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 15:24:41,851]: Mean: -0.00000137
[2025-06-12 15:24:41,851]: Min: -0.04255720
[2025-06-12 15:24:41,852]: Max: 0.03723755
[2025-06-12 15:24:41,852]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-06-12 15:24:41,853]: Sample Values (25 elements): [0.03454776480793953, 2.5614881451474503e-05, 3.6466938126977766e-06, -1.3343626051209867e-05, 3.73645749412402e-32, 1.0404275599285029e-05, -8.856236299870091e-10, 0.0025008900556713343, 2.723017314565368e-05, 8.718229758182972e-15, -1.2204756785649806e-05, 6.854423190816306e-06, 0.008427295833826065, 0.00028579437639564276, -9.664370502876981e-20, 6.077711699469597e-41, 6.249791150888684e-41, 5.373113526729867e-05, -1.663093058823506e-07, -6.049965989875965e-41, 5.796611227526038e-41, 6.220657814992592e-05, -6.746235499122122e-07, -3.362825680142123e-07, -2.822280293912627e-07]
[2025-06-12 15:24:41,853]: Mean: 0.00757795
[2025-06-12 15:24:41,853]: Min: -0.00219047
[2025-06-12 15:24:41,853]: Max: 0.12922010
[2025-06-12 15:24:41,853]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-06-12 15:24:41,854]: Sample Values (25 elements): [4.907207092219077e-41, -0.04554340988397598, -0.05955122411251068, -0.08441467583179474, 0.20339319109916687, -0.14670361578464508, -0.004618512000888586, -0.06521773338317871, -0.13017615675926208, -0.009770002216100693, 0.04531364142894745, 0.10773975402116776, 0.09873165935277939, -0.1605309247970581, 3.116833613603376e-05, 0.036818310618400574, 0.0798775926232338, -0.0850781723856926, 6.167818700220096e-09, -3.5850091535394313e-06, 0.15917885303497314, 5.642748656143173e-41, -0.07754970341920853, 0.00870546419173479, 0.12391403317451477]
[2025-06-12 15:24:41,854]: Mean: -0.02033695
[2025-06-12 15:24:41,854]: Min: -0.62280124
[2025-06-12 15:24:41,854]: Max: 0.37079197
[2025-06-12 15:24:41,854]: 


QAT of ResNet18 with parametrized_relu down to 3 bits...
[2025-06-12 15:24:42,079]: [ResNet18_parametrized_relu_quantized_3_bits] after configure_qat:
[2025-06-12 15:24:42,107]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-06-12 15:26:49,356]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 001 Train Loss: 0.2291 Train Acc: 0.9195 Eval Loss: 0.3678 Eval Acc: 0.8889 (LR: 0.00100000)
[2025-06-12 15:29:02,652]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 002 Train Loss: 0.2340 Train Acc: 0.9187 Eval Loss: 0.3900 Eval Acc: 0.8786 (LR: 0.00100000)
[2025-06-12 15:30:48,953]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 003 Train Loss: 0.2371 Train Acc: 0.9165 Eval Loss: 0.3970 Eval Acc: 0.8803 (LR: 0.00100000)
[2025-06-12 15:32:33,538]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 004 Train Loss: 0.2429 Train Acc: 0.9153 Eval Loss: 0.3835 Eval Acc: 0.8831 (LR: 0.00100000)
[2025-06-12 15:34:21,341]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 005 Train Loss: 0.2456 Train Acc: 0.9144 Eval Loss: 0.4213 Eval Acc: 0.8723 (LR: 0.00100000)
[2025-06-12 15:36:12,425]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 006 Train Loss: 0.2415 Train Acc: 0.9160 Eval Loss: 0.3525 Eval Acc: 0.8863 (LR: 0.00100000)
[2025-06-12 15:38:05,925]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 007 Train Loss: 0.2448 Train Acc: 0.9143 Eval Loss: 0.4624 Eval Acc: 0.8646 (LR: 0.00100000)
[2025-06-12 15:39:52,832]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 008 Train Loss: 0.2424 Train Acc: 0.9156 Eval Loss: 0.3880 Eval Acc: 0.8812 (LR: 0.00100000)
[2025-06-12 15:41:43,764]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 009 Train Loss: 0.2453 Train Acc: 0.9149 Eval Loss: 0.3953 Eval Acc: 0.8786 (LR: 0.00100000)
[2025-06-12 15:43:33,116]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 010 Train Loss: 0.2443 Train Acc: 0.9154 Eval Loss: 0.3991 Eval Acc: 0.8756 (LR: 0.00100000)
[2025-06-12 15:45:22,354]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 011 Train Loss: 0.2383 Train Acc: 0.9167 Eval Loss: 0.3956 Eval Acc: 0.8798 (LR: 0.00100000)
[2025-06-12 15:47:11,600]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 012 Train Loss: 0.2395 Train Acc: 0.9185 Eval Loss: 0.3053 Eval Acc: 0.9047 (LR: 0.00100000)
[2025-06-12 15:48:58,986]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 013 Train Loss: 0.2426 Train Acc: 0.9150 Eval Loss: 0.3471 Eval Acc: 0.8902 (LR: 0.00100000)
[2025-06-12 15:50:45,471]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 014 Train Loss: 0.2378 Train Acc: 0.9165 Eval Loss: 0.3559 Eval Acc: 0.8880 (LR: 0.00100000)
[2025-06-12 15:52:32,051]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 015 Train Loss: 0.2361 Train Acc: 0.9179 Eval Loss: 0.3247 Eval Acc: 0.9002 (LR: 0.00100000)
[2025-06-12 15:54:18,797]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 016 Train Loss: 0.2377 Train Acc: 0.9162 Eval Loss: 0.4111 Eval Acc: 0.8719 (LR: 0.00100000)
[2025-06-12 15:56:05,612]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 017 Train Loss: 0.2375 Train Acc: 0.9178 Eval Loss: 0.3614 Eval Acc: 0.8922 (LR: 0.00100000)
[2025-06-12 15:57:51,660]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 018 Train Loss: 0.2336 Train Acc: 0.9188 Eval Loss: 0.3598 Eval Acc: 0.8877 (LR: 0.00010000)
[2025-06-12 15:59:36,997]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 019 Train Loss: 0.1560 Train Acc: 0.9467 Eval Loss: 0.2461 Eval Acc: 0.9228 (LR: 0.00010000)
[2025-06-12 16:01:22,125]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 020 Train Loss: 0.1261 Train Acc: 0.9566 Eval Loss: 0.2458 Eval Acc: 0.9234 (LR: 0.00010000)
[2025-06-12 16:03:06,845]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 021 Train Loss: 0.1161 Train Acc: 0.9593 Eval Loss: 0.2398 Eval Acc: 0.9267 (LR: 0.00010000)
[2025-06-12 16:04:51,340]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 022 Train Loss: 0.1084 Train Acc: 0.9634 Eval Loss: 0.2365 Eval Acc: 0.9299 (LR: 0.00010000)
[2025-06-12 16:06:35,671]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 023 Train Loss: 0.1045 Train Acc: 0.9642 Eval Loss: 0.2441 Eval Acc: 0.9267 (LR: 0.00010000)
[2025-06-12 16:08:19,977]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 024 Train Loss: 0.1005 Train Acc: 0.9657 Eval Loss: 0.2452 Eval Acc: 0.9297 (LR: 0.00010000)
[2025-06-12 16:10:04,152]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 025 Train Loss: 0.0948 Train Acc: 0.9668 Eval Loss: 0.2419 Eval Acc: 0.9321 (LR: 0.00010000)
[2025-06-12 16:11:48,301]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 026 Train Loss: 0.0945 Train Acc: 0.9677 Eval Loss: 0.2422 Eval Acc: 0.9300 (LR: 0.00010000)
[2025-06-12 16:13:32,398]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 027 Train Loss: 0.0903 Train Acc: 0.9689 Eval Loss: 0.2587 Eval Acc: 0.9295 (LR: 0.00010000)
[2025-06-12 16:15:15,653]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 028 Train Loss: 0.0845 Train Acc: 0.9708 Eval Loss: 0.2451 Eval Acc: 0.9283 (LR: 0.00001000)
[2025-06-12 16:17:03,324]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 029 Train Loss: 0.0765 Train Acc: 0.9740 Eval Loss: 0.2424 Eval Acc: 0.9323 (LR: 0.00001000)
[2025-06-12 16:18:48,019]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 030 Train Loss: 0.0734 Train Acc: 0.9752 Eval Loss: 0.2361 Eval Acc: 0.9363 (LR: 0.00001000)
[2025-06-12 16:20:32,559]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 031 Train Loss: 0.0741 Train Acc: 0.9751 Eval Loss: 0.2437 Eval Acc: 0.9327 (LR: 0.00001000)
[2025-06-12 16:22:16,940]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 032 Train Loss: 0.0707 Train Acc: 0.9765 Eval Loss: 0.2307 Eval Acc: 0.9333 (LR: 0.00001000)
[2025-06-12 16:24:01,871]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 033 Train Loss: 0.0710 Train Acc: 0.9756 Eval Loss: 0.2385 Eval Acc: 0.9337 (LR: 0.00001000)
[2025-06-12 16:25:46,184]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 034 Train Loss: 0.0688 Train Acc: 0.9763 Eval Loss: 0.2356 Eval Acc: 0.9352 (LR: 0.00001000)
[2025-06-12 16:27:30,496]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 035 Train Loss: 0.0687 Train Acc: 0.9767 Eval Loss: 0.2399 Eval Acc: 0.9337 (LR: 0.00001000)
[2025-06-12 16:29:15,037]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 036 Train Loss: 0.0667 Train Acc: 0.9770 Eval Loss: 0.2372 Eval Acc: 0.9344 (LR: 0.00001000)
[2025-06-12 16:30:59,378]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 037 Train Loss: 0.0687 Train Acc: 0.9769 Eval Loss: 0.2395 Eval Acc: 0.9334 (LR: 0.00001000)
[2025-06-12 16:33:05,923]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 038 Train Loss: 0.0670 Train Acc: 0.9773 Eval Loss: 0.2386 Eval Acc: 0.9332 (LR: 0.00000100)
[2025-06-12 16:35:12,756]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 039 Train Loss: 0.0660 Train Acc: 0.9775 Eval Loss: 0.2453 Eval Acc: 0.9341 (LR: 0.00000100)
[2025-06-12 16:37:20,194]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 040 Train Loss: 0.0657 Train Acc: 0.9773 Eval Loss: 0.2376 Eval Acc: 0.9343 (LR: 0.00000100)
[2025-06-12 16:39:27,260]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 041 Train Loss: 0.0679 Train Acc: 0.9769 Eval Loss: 0.2355 Eval Acc: 0.9346 (LR: 0.00000100)
[2025-06-12 16:41:33,929]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 042 Train Loss: 0.0660 Train Acc: 0.9779 Eval Loss: 0.2379 Eval Acc: 0.9366 (LR: 0.00000100)
[2025-06-12 16:43:40,856]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 043 Train Loss: 0.0626 Train Acc: 0.9789 Eval Loss: 0.2327 Eval Acc: 0.9362 (LR: 0.00000100)
[2025-06-12 16:45:47,839]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 044 Train Loss: 0.0635 Train Acc: 0.9787 Eval Loss: 0.2370 Eval Acc: 0.9327 (LR: 0.00000010)
[2025-06-12 16:47:54,710]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 045 Train Loss: 0.0646 Train Acc: 0.9779 Eval Loss: 0.2384 Eval Acc: 0.9339 (LR: 0.00000010)
[2025-06-12 16:50:01,802]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 046 Train Loss: 0.0647 Train Acc: 0.9775 Eval Loss: 0.2393 Eval Acc: 0.9342 (LR: 0.00000010)
[2025-06-12 16:52:17,793]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 047 Train Loss: 0.0659 Train Acc: 0.9771 Eval Loss: 0.2378 Eval Acc: 0.9340 (LR: 0.00000010)
[2025-06-12 16:54:39,633]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 048 Train Loss: 0.0637 Train Acc: 0.9779 Eval Loss: 0.2400 Eval Acc: 0.9341 (LR: 0.00000010)
[2025-06-12 16:57:01,337]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 049 Train Loss: 0.0633 Train Acc: 0.9784 Eval Loss: 0.2355 Eval Acc: 0.9358 (LR: 0.00000010)
[2025-06-12 16:59:22,843]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 050 Train Loss: 0.0634 Train Acc: 0.9788 Eval Loss: 0.2405 Eval Acc: 0.9325 (LR: 0.00000010)
[2025-06-12 17:01:44,162]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 051 Train Loss: 0.0643 Train Acc: 0.9784 Eval Loss: 0.2352 Eval Acc: 0.9347 (LR: 0.00000010)
[2025-06-12 17:04:05,323]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 052 Train Loss: 0.0645 Train Acc: 0.9780 Eval Loss: 0.2392 Eval Acc: 0.9347 (LR: 0.00000010)
[2025-06-12 17:06:27,131]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 053 Train Loss: 0.0650 Train Acc: 0.9773 Eval Loss: 0.2406 Eval Acc: 0.9337 (LR: 0.00000010)
[2025-06-12 17:08:48,371]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 054 Train Loss: 0.0613 Train Acc: 0.9792 Eval Loss: 0.2407 Eval Acc: 0.9349 (LR: 0.00000010)
[2025-06-12 17:11:10,307]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 055 Train Loss: 0.0625 Train Acc: 0.9793 Eval Loss: 0.2391 Eval Acc: 0.9337 (LR: 0.00000010)
[2025-06-12 17:13:32,200]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 056 Train Loss: 0.0649 Train Acc: 0.9777 Eval Loss: 0.2408 Eval Acc: 0.9337 (LR: 0.00000010)
[2025-06-12 17:15:54,230]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 057 Train Loss: 0.0631 Train Acc: 0.9784 Eval Loss: 0.2344 Eval Acc: 0.9348 (LR: 0.00000010)
[2025-06-12 17:18:16,323]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 058 Train Loss: 0.0636 Train Acc: 0.9790 Eval Loss: 0.2381 Eval Acc: 0.9356 (LR: 0.00000010)
[2025-06-12 17:20:38,140]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 059 Train Loss: 0.0627 Train Acc: 0.9790 Eval Loss: 0.2387 Eval Acc: 0.9350 (LR: 0.00000010)
[2025-06-12 17:23:00,906]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 060 Train Loss: 0.0634 Train Acc: 0.9787 Eval Loss: 0.2369 Eval Acc: 0.9341 (LR: 0.00000010)
[2025-06-12 17:23:00,907]: [ResNet18_parametrized_relu_quantized_3_bits] Best Eval Accuracy: 0.9366
[2025-06-12 17:23:01,028]: 


Quantization of model down to 3 bits finished
[2025-06-12 17:23:01,029]: Model Architecture:
[2025-06-12 17:23:01,091]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3024], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.1171488761901855)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1274], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.48550155758857727, max_val=0.40612030029296875)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2677], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1213], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.38385826349258423, max_val=0.46540477871894836)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4131], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1269], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.41791629791259766, max_val=0.47007113695144653)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2351], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0936], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.327728807926178, max_val=0.3277880549430847)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4388], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1083], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.39223021268844604, max_val=0.36608028411865234)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3011], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0877], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27465391159057617, max_val=0.3391134738922119)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1581], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5575019121170044, max_val=0.5492386817932129)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0898], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30636143684387207, max_val=0.3223673701286316)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2820], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0761], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2510603070259094, max_val=0.28146421909332275)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4125], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0835], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25785398483276367, max_val=0.32660973072052)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3045], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0936], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25232118368148804, max_val=0.4027329683303833)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1170], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3449863791465759, max_val=0.47371596097946167)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3445], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0866], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25167351961135864, max_val=0.3544914126396179)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2572], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0788], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2642063796520233, max_val=0.28725624084472656)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3549], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0803], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2370193898677826, max_val=0.32511866092681885)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3285], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0548], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17667508125305176, max_val=0.20723754167556763)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0702], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2167612761259079, max_val=0.27470219135284424)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3582], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0002], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.000620886858087033, max_val=0.0008647929644212127)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0126], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0004], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0013505963142961264, max_val=0.0013361293822526932)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3583], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-06-12 17:23:01,091]: 
Model Weights:
[2025-06-12 17:23:01,091]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-06-12 17:23:01,092]: Sample Values (25 elements): [-0.10308025032281876, -0.2664567530155182, -0.2825784683227539, -0.2063637673854828, -0.0018677577609196305, -0.2517230212688446, 0.026610827073454857, -0.06952937692403793, -0.062444526702165604, -0.21776708960533142, -0.04119856283068657, -0.06516536325216293, -0.12853580713272095, -0.174587219953537, -0.15577560663223267, 0.042846690863370895, -0.059999335557222366, 0.060173287987709045, 0.22707675397396088, 0.1346661001443863, 0.0063571264035999775, 0.19737394154071808, 0.19890142977237701, -0.10211782902479172, -0.1995384246110916]
[2025-06-12 17:23:01,092]: Mean: -0.00131183
[2025-06-12 17:23:01,093]: Min: -0.46529007
[2025-06-12 17:23:01,093]: Max: 0.42024952
[2025-06-12 17:23:01,093]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-06-12 17:23:01,093]: Sample Values (25 elements): [0.8080480098724365, 1.352030873298645, 0.6711891293525696, 0.7052004933357239, 0.46877074241638184, 0.6704602837562561, 0.5032321214675903, 0.1937166303396225, 0.7426312565803528, 1.2531803846359253, 0.849895179271698, 0.6949287056922913, 0.3500247895717621, 0.6247966885566711, 0.5196945071220398, 0.5593330264091492, 1.0260330438613892, 0.764234185218811, 0.6485409736633301, 0.963627815246582, 0.11755604296922684, 0.6029200553894043, 0.6163396835327148, 0.4460608661174774, 0.5826680064201355]
[2025-06-12 17:23:01,094]: Mean: 0.67415392
[2025-06-12 17:23:01,094]: Min: 0.11755604
[2025-06-12 17:23:01,094]: Max: 1.35203087
[2025-06-12 17:23:01,096]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 17:23:01,097]: Sample Values (25 elements): [-0.1273745596408844, -0.1273745596408844, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1273745596408844, -0.1273745596408844, 0.0, 0.0, 0.0, 0.0, 0.1273745596408844, 0.0, 0.1273745596408844, 0.0, -0.1273745596408844, -0.2547491192817688, 0.1273745596408844, 0.0, -0.1273745596408844, 0.0, 0.0, 0.0]
[2025-06-12 17:23:01,097]: Mean: -0.00684832
[2025-06-12 17:23:01,097]: Min: -0.50949824
[2025-06-12 17:23:01,097]: Max: 0.38212368
[2025-06-12 17:23:01,097]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-06-12 17:23:01,098]: Sample Values (25 elements): [0.5644181966781616, 0.6390630006790161, 0.6141164302825928, 0.6701819896697998, 0.5077970027923584, 0.5669988989830017, 0.43409180641174316, 0.3906080722808838, 0.5736106038093567, 0.8061321377754211, 0.7750553488731384, 0.34590569138526917, 0.3258322775363922, 0.6133701205253601, 0.7352948784828186, 0.5747336149215698, 0.6482276916503906, 0.279609352350235, 0.5537514686584473, 0.9653151035308838, 0.39592838287353516, 0.3727211356163025, 0.629114031791687, 0.6443024277687073, 0.7077967524528503]
[2025-06-12 17:23:01,098]: Mean: 0.60393560
[2025-06-12 17:23:01,098]: Min: 0.27960935
[2025-06-12 17:23:01,098]: Max: 0.98833150
[2025-06-12 17:23:01,100]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 17:23:01,100]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.12132330238819122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 17:23:01,101]: Mean: -0.00138885
[2025-06-12 17:23:01,101]: Min: -0.36396992
[2025-06-12 17:23:01,101]: Max: 0.48529321
[2025-06-12 17:23:01,101]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-06-12 17:23:01,101]: Sample Values (25 elements): [0.5325098037719727, 0.7093952298164368, 0.4739989936351776, 0.5383493900299072, 0.48039278388023376, 0.6871205568313599, 0.7872591018676758, 0.5469684600830078, 0.9703023433685303, 0.5689113140106201, 0.3250984251499176, 1.2049367427825928, 0.6167331337928772, 0.3548082113265991, 0.4171839952468872, 0.7764502763748169, 0.5191620588302612, 0.6430861353874207, 0.6365450620651245, 0.6581618189811707, 0.7878211140632629, 0.5295218825340271, 0.3541927933692932, 0.95427006483078, 0.6361936330795288]
[2025-06-12 17:23:01,101]: Mean: 0.68826848
[2025-06-12 17:23:01,102]: Min: 0.23191635
[2025-06-12 17:23:01,102]: Max: 1.26212204
[2025-06-12 17:23:01,103]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 17:23:01,104]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12685535848140717, -0.12685535848140717, 0.0, 0.0, 0.0, -0.12685535848140717, 0.0, 0.0, 0.0, 0.0, 0.0, -0.12685535848140717, -0.12685535848140717, 0.0, 0.0, 0.0, 0.12685535848140717, 0.0]
[2025-06-12 17:23:01,104]: Mean: -0.00352376
[2025-06-12 17:23:01,104]: Min: -0.38056606
[2025-06-12 17:23:01,104]: Max: 0.50742143
[2025-06-12 17:23:01,104]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-06-12 17:23:01,105]: Sample Values (25 elements): [0.6193526983261108, 1.027964472770691, 0.5287606120109558, 0.5254182815551758, 0.43447181582450867, 0.5932145118713379, 0.4466395974159241, 0.7522419691085815, 0.4216713011264801, 0.7193873524665833, 0.298553466796875, 0.553906261920929, 0.27198177576065063, 0.6511625647544861, 0.32090696692466736, 0.43014565110206604, 0.506328821182251, 0.5092277526855469, 0.5332456827163696, 0.3236684203147888, 0.5344258546829224, 0.9723958969116211, 0.6760019659996033, 0.6971409916877747, 0.4546118676662445]
[2025-06-12 17:23:01,105]: Mean: 0.55491632
[2025-06-12 17:23:01,105]: Min: 0.00031527
[2025-06-12 17:23:01,105]: Max: 1.10946369
[2025-06-12 17:23:01,106]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 17:23:01,107]: Sample Values (25 elements): [-0.09364526718854904, 0.09364526718854904, -0.09364526718854904, -0.09364526718854904, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09364526718854904, 0.0, 0.0, 0.0, 0.09364526718854904, 0.0, 0.0, 0.09364526718854904, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 17:23:01,107]: Mean: -0.00270033
[2025-06-12 17:23:01,107]: Min: -0.28093579
[2025-06-12 17:23:01,108]: Max: 0.37458107
[2025-06-12 17:23:01,108]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-06-12 17:23:01,108]: Sample Values (25 elements): [0.505933403968811, 0.48631051182746887, 0.6175370812416077, 0.6756466627120972, 0.4926277697086334, 0.5259567499160767, 0.592512309551239, 0.8061831593513489, 0.646897554397583, 0.3949132561683655, 0.6602116823196411, 0.5039638876914978, 0.4439835250377655, 0.8762794733047485, 0.6720033288002014, 0.7102885246276855, 0.4971159100532532, 0.8219295740127563, 0.8369867205619812, 0.4101351797580719, 0.739460289478302, 0.8583642244338989, 0.6657036542892456, 0.8379396796226501, 0.8678549528121948]
[2025-06-12 17:23:01,108]: Mean: 0.62570840
[2025-06-12 17:23:01,108]: Min: 0.17583679
[2025-06-12 17:23:01,109]: Max: 0.92827320
[2025-06-12 17:23:01,110]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-06-12 17:23:01,111]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10833007842302322, 0.0, -0.10833007842302322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21666015684604645, 0.10833007842302322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.21666015684604645]
[2025-06-12 17:23:01,111]: Mean: -0.00129300
[2025-06-12 17:23:01,112]: Min: -0.43332031
[2025-06-12 17:23:01,112]: Max: 0.32499024
[2025-06-12 17:23:01,112]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-06-12 17:23:01,112]: Sample Values (25 elements): [0.4968574345111847, 0.46340644359588623, 0.7129020690917969, 0.6286978125572205, 0.5417779684066772, 0.656654953956604, 0.24176594614982605, 0.3929460048675537, 0.3629346787929535, 0.5338282585144043, 0.6387882232666016, 0.5373559594154358, 0.49793094396591187, -4.942519813520062e-41, 0.5210874676704407, 0.628233015537262, 0.6143732666969299, 0.5230162143707275, 0.6015098690986633, 0.12377841025590897, 0.39529359340667725, 0.47629985213279724, 0.7439396977424622, 0.6361026763916016, 0.5579925775527954]
[2025-06-12 17:23:01,112]: Mean: 0.46869758
[2025-06-12 17:23:01,113]: Min: -0.00000000
[2025-06-12 17:23:01,113]: Max: 0.92201996
[2025-06-12 17:23:01,114]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-12 17:23:01,116]: Sample Values (25 elements): [0.0, 0.08768106251955032, 0.08768106251955032, 0.0, 0.0, 0.08768106251955032, 0.0, 0.0, 0.08768106251955032, 0.0, 0.0, 0.0, -0.08768106251955032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08768106251955032, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 17:23:01,116]: Mean: -0.00218406
[2025-06-12 17:23:01,116]: Min: -0.26304320
[2025-06-12 17:23:01,116]: Max: 0.35072425
[2025-06-12 17:23:01,116]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-06-12 17:23:01,117]: Sample Values (25 elements): [0.6410364508628845, 5.276235242490657e-05, 0.7142853140830994, 0.6229650378227234, 0.5858906507492065, 0.5459161996841431, 0.6108397245407104, 0.5176566243171692, 0.5098412036895752, 0.6264896392822266, 0.9665707945823669, 0.7018179893493652, 0.5973480939865112, 0.206133171916008, 0.5091659426689148, 0.4308528006076813, 0.665858805179596, 0.43457552790641785, 0.526657223701477, 0.4922131597995758, 0.7585472464561462, 0.6753656268119812, 0.6885048747062683, 0.5474498271942139, 0.4623209536075592]
[2025-06-12 17:23:01,118]: Mean: 0.60523742
[2025-06-12 17:23:01,118]: Min: 0.00005276
[2025-06-12 17:23:01,118]: Max: 0.96657079
[2025-06-12 17:23:01,119]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-06-12 17:23:01,120]: Sample Values (25 elements): [0.0, -0.15810580551624298, 0.15810580551624298, 0.0, 0.15810580551624298, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.15810580551624298, 0.0, 0.15810580551624298, 0.0, 0.15810580551624298, -0.15810580551624298, 0.0, 0.15810580551624298, 0.0, 0.31621161103248596, 0.0, -0.15810580551624298, 0.15810580551624298, -0.15810580551624298]
[2025-06-12 17:23:01,120]: Mean: -0.00254760
[2025-06-12 17:23:01,120]: Min: -0.63242322
[2025-06-12 17:23:01,120]: Max: 0.47431743
[2025-06-12 17:23:01,121]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-06-12 17:23:01,121]: Sample Values (25 elements): [0.34288379549980164, 0.4564015865325928, 0.43924248218536377, 0.25318223237991333, 0.29907551407814026, 0.45453372597694397, 0.45205381512641907, 0.6800732016563416, 0.5393784642219543, 0.4928116500377655, 0.30255842208862305, 0.5580057501792908, 0.07111856341362, 0.511020302772522, 0.4459175765514374, 0.46423768997192383, 0.6114133596420288, 0.28846481442451477, 0.5616539716720581, 0.5256989002227783, 0.5046601295471191, 0.3854481875896454, 0.35727936029434204, 0.4267578125, 0.48391193151474]
[2025-06-12 17:23:01,121]: Mean: 0.41845804
[2025-06-12 17:23:01,121]: Min: 0.00002506
[2025-06-12 17:23:01,121]: Max: 0.70277148
[2025-06-12 17:23:01,122]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-12 17:23:01,124]: Sample Values (25 elements): [0.0, 0.0, -0.08981840312480927, 0.08981840312480927, 0.0, -0.08981840312480927, 0.0, 0.0, 0.0, -0.08981840312480927, 0.08981840312480927, 0.0, 0.08981840312480927, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 17:23:01,124]: Mean: -0.00260947
[2025-06-12 17:23:01,124]: Min: -0.26945519
[2025-06-12 17:23:01,124]: Max: 0.35927361
[2025-06-12 17:23:01,125]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-06-12 17:23:01,125]: Sample Values (25 elements): [0.40933758020401, 0.5236920118331909, 0.5702109932899475, 0.12257018685340881, 0.5017150044441223, 0.4332157373428345, -4.936213970430601e-41, 4.673608145822072e-06, 0.558357298374176, 0.5070506930351257, 0.4680848717689514, 0.33365046977996826, 0.5103373527526855, 0.10979639738798141, 0.5407339930534363, 0.5370609164237976, 0.3053564131259918, 0.43987059593200684, 0.6065835952758789, 0.5214128494262695, 0.6222826838493347, 0.44557619094848633, 0.5741360783576965, 0.4592382311820984, 4.916735921776486e-41]
[2025-06-12 17:23:01,125]: Mean: 0.41911936
[2025-06-12 17:23:01,125]: Min: -0.00000000
[2025-06-12 17:23:01,125]: Max: 0.76667559
[2025-06-12 17:23:01,126]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-12 17:23:01,128]: Sample Values (25 elements): [0.0, 0.0, -0.07607493549585342, 0.0, 0.0, -0.07607493549585342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07607493549585342, 0.0, 0.07607493549585342, -0.07607493549585342, 0.07607493549585342, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 17:23:01,128]: Mean: -0.00238560
[2025-06-12 17:23:01,129]: Min: -0.22822481
[2025-06-12 17:23:01,129]: Max: 0.30429974
[2025-06-12 17:23:01,129]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-06-12 17:23:01,129]: Sample Values (25 elements): [0.6358307600021362, 0.3215561509132385, 0.47711434960365295, 0.26855742931365967, 0.6403834819793701, 0.3771377205848694, 0.5599073767662048, 0.6252845525741577, 0.6940010786056519, 0.49361085891723633, 0.6416714191436768, 0.3811838924884796, 0.49238622188568115, 0.5108602046966553, 0.5043412446975708, 0.4781181216239929, 0.021217113360762596, 0.25731611251831055, 0.09573590010404587, 0.46998196840286255, 0.33777520060539246, 0.37000781297683716, 0.41956010460853577, 0.43561893701553345, 0.4809471368789673]
[2025-06-12 17:23:01,130]: Mean: 0.43634832
[2025-06-12 17:23:01,130]: Min: -0.00047209
[2025-06-12 17:23:01,130]: Max: 0.79408330
[2025-06-12 17:23:01,131]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-06-12 17:23:01,135]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.08349481970071793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.08349481970071793]
[2025-06-12 17:23:01,135]: Mean: -0.00048979
[2025-06-12 17:23:01,135]: Min: -0.25048447
[2025-06-12 17:23:01,135]: Max: 0.33397928
[2025-06-12 17:23:01,136]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-06-12 17:23:01,136]: Sample Values (25 elements): [0.2759090065956116, 5.259073136611038e-41, 0.27530163526535034, 0.4390694797039032, -5.8577078405706e-41, 0.402692586183548, 0.3762702941894531, -5.170370943819278e-41, 0.2714534103870392, -5.595945287434724e-41, -5.257391578453849e-41, 0.39776477217674255, 0.47193342447280884, 0.5087823271751404, 4.910570208533456e-41, 4.905245274369022e-41, 0.4426000714302063, 0.499769002199173, 0.5873201489448547, 0.48855531215667725, 0.485890656709671, 4.941118515055737e-41, 0.48062336444854736, 0.10282731056213379, -5.034725252472635e-41]
[2025-06-12 17:23:01,136]: Mean: 0.28819686
[2025-06-12 17:23:01,136]: Min: -0.00000000
[2025-06-12 17:23:01,136]: Max: 0.77768165
[2025-06-12 17:23:01,138]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-12 17:23:01,145]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09357916563749313, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 17:23:01,145]: Mean: -0.00058132
[2025-06-12 17:23:01,146]: Min: -0.28073749
[2025-06-12 17:23:01,146]: Max: 0.37431666
[2025-06-12 17:23:01,146]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-06-12 17:23:01,146]: Sample Values (25 elements): [0.0007464644149877131, 0.21387769281864166, 0.3545936346054077, 0.4826616942882538, 0.13718281686306, 0.5703758001327515, 0.5826665759086609, 0.4754706025123596, 0.736400306224823, 6.086539879794843e-41, 0.3672407567501068, 0.48139798641204834, 0.5077937841415405, 0.5943946242332458, 5.176957046601604e-41, -5.664889171879505e-41, 0.49980929493904114, 0.5260887742042542, 0.3789623975753784, 0.6357635259628296, 0.3244974911212921, 0.5515278577804565, 0.3558793365955353, -4.92430293348384e-41, 0.4185682237148285]
[2025-06-12 17:23:01,146]: Mean: 0.38581476
[2025-06-12 17:23:01,146]: Min: -0.00000000
[2025-06-12 17:23:01,147]: Max: 0.92059451
[2025-06-12 17:23:01,148]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-06-12 17:23:01,149]: Sample Values (25 elements): [0.0, 0.11695748567581177, 0.0, 0.0, 0.0, 0.11695748567581177, 0.0, -0.11695748567581177, 0.0, 0.0, -0.11695748567581177, 0.0, 0.0, 0.0, 0.11695748567581177, 0.0, 0.11695748567581177, 0.11695748567581177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 17:23:01,149]: Mean: -0.00328015
[2025-06-12 17:23:01,149]: Min: -0.35087246
[2025-06-12 17:23:01,150]: Max: 0.46782994
[2025-06-12 17:23:01,150]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-06-12 17:23:01,150]: Sample Values (25 elements): [0.3252604305744171, 0.0010349673684686422, 0.26328805088996887, 0.20387813448905945, 0.24043206870555878, 0.22650064527988434, 0.0064960867166519165, 0.37491029500961304, 0.22162851691246033, 0.4866463541984558, 0.40336182713508606, 0.26560136675834656, 0.305303692817688, 0.3000814914703369, -5.022674085679442e-41, 0.24746380746364594, -5.220537428842106e-41, 0.31802263855934143, 5.637984241364469e-41, 0.37470170855522156, 0.22427643835544586, 0.2826713025569916, 0.26631054282188416, 0.2591502368450165, 3.3418332493616896e-18]
[2025-06-12 17:23:01,150]: Mean: 0.20634788
[2025-06-12 17:23:01,151]: Min: -0.02772394
[2025-06-12 17:23:01,151]: Max: 0.48664635
[2025-06-12 17:23:01,152]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-12 17:23:01,158]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.08659499138593674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 17:23:01,158]: Mean: -0.00068665
[2025-06-12 17:23:01,158]: Min: -0.25978497
[2025-06-12 17:23:01,159]: Max: 0.34637997
[2025-06-12 17:23:01,159]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-06-12 17:23:01,159]: Sample Values (25 elements): [9.019237949275254e-23, 0.41040053963661194, -5.966588731248639e-41, 5.356463379881613e-41, 0.39752689003944397, 5.565817370451741e-41, 0.29229602217674255, 5.152294193629487e-41, 0.5346481800079346, -5.219416390070646e-41, 0.457568496465683, 0.12935557961463928, 6.289728157121941e-41, -5.084891737495464e-41, 4.929067348262544e-41, 0.5166530609130859, 0.5648555159568787, 0.5518732070922852, 5.845096154391677e-41, 0.462830126285553, 0.30529600381851196, 0.004975354298949242, 6.199624665865856e-41, -4.928086439337517e-41, -4.912251766690646e-41]
[2025-06-12 17:23:01,159]: Mean: 0.15857933
[2025-06-12 17:23:01,159]: Min: -0.00000000
[2025-06-12 17:23:01,159]: Max: 0.69504285
[2025-06-12 17:23:01,161]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-12 17:23:01,168]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.07878037542104721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07878037542104721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07878037542104721, 0.0, 0.0, 0.0]
[2025-06-12 17:23:01,168]: Mean: -0.00053266
[2025-06-12 17:23:01,168]: Min: -0.23634112
[2025-06-12 17:23:01,169]: Max: 0.31512150
[2025-06-12 17:23:01,169]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-06-12 17:23:01,169]: Sample Values (25 elements): [0.5021917819976807, -4.922200985787352e-41, 0.506216824054718, -5.74196058741737e-41, 0.46694839000701904, 0.2794897258281708, 0.4026530086994171, 0.108658567070961, -0.002867179224267602, 5.209046781434642e-41, 5.907734195746996e-41, 0.10524825006723404, 0.10234249383211136, 0.10733325779438019, 0.4255309998989105, 0.22187907993793488, 0.09719468653202057, 0.04463715851306915, 0.03327362239360809, 0.0006243875832296908, 0.6542604565620422, 0.05872448533773422, 5.150192245933e-41, 0.056507501751184464, 0.2829242944717407]
[2025-06-12 17:23:01,169]: Mean: 0.19795658
[2025-06-12 17:23:01,169]: Min: -0.01443781
[2025-06-12 17:23:01,170]: Max: 0.66438860
[2025-06-12 17:23:01,171]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-06-12 17:23:01,195]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 17:23:01,196]: Mean: -0.00007590
[2025-06-12 17:23:01,196]: Min: -0.24091633
[2025-06-12 17:23:01,196]: Max: 0.32122177
[2025-06-12 17:23:01,196]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-06-12 17:23:01,197]: Sample Values (25 elements): [0.15094199776649475, 4.906086053447617e-41, -4.904684754983292e-41, -5.435216353576668e-41, -5.204002106963073e-41, -6.233676218548949e-41, 5.874943811681796e-41, -4.925283842408867e-41, 0.6954240202903748, -4.923602284251677e-41, 0.34423333406448364, 5.409012072293794e-41, 0.3656698763370514, -4.904684754983292e-41, -4.936634359969898e-41, -4.912251766690646e-41, -5.220537428842106e-41, 4.951908513231039e-41, 1.1152437195960374e-07, 5.162663802265491e-41, 0.5075141191482544, 4.944902020909414e-41, -5.975977430959615e-41, -4.908888650376267e-41, -5.252907423368009e-41]
[2025-06-12 17:23:01,197]: Mean: 0.04573491
[2025-06-12 17:23:01,198]: Min: -0.00000000
[2025-06-12 17:23:01,198]: Max: 0.86953747
[2025-06-12 17:23:01,199]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-12 17:23:01,275]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 17:23:01,275]: Mean: 0.00006548
[2025-06-12 17:23:01,276]: Min: -0.16453397
[2025-06-12 17:23:01,276]: Max: 0.21937864
[2025-06-12 17:23:01,276]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-06-12 17:23:01,276]: Sample Values (25 elements): [0.6572760343551636, 0.6714075803756714, 0.3364928364753723, 5.312322478255382e-41, 0.036970097571611404, -5.187747044776905e-41, -5.497714265085555e-41, 0.06187933310866356, 0.7108551859855652, 6.1463077827284e-07, -5.67077462542967e-41, 0.30881956219673157, 0.5942631959915161, 5.764964505772241e-09, 0.5106773376464844, 0.21498824656009674, 0.4525010883808136, 0.26974859833717346, 0.05351470038294792, 0.6538228988647461, -4.922481245480217e-41, 0.6065449118614197, 0.2910163104534149, 0.451143741607666, -4.907907741451239e-41]
[2025-06-12 17:23:01,277]: Mean: 0.25080121
[2025-06-12 17:23:01,277]: Min: -0.00002815
[2025-06-12 17:23:01,277]: Max: 0.84118509
[2025-06-12 17:23:01,278]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-06-12 17:23:01,280]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 17:23:01,281]: Mean: 0.00045316
[2025-06-12 17:23:01,281]: Min: -0.21062720
[2025-06-12 17:23:01,281]: Max: 0.28083625
[2025-06-12 17:23:01,281]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-06-12 17:23:01,282]: Sample Values (25 elements): [0.48464033007621765, 0.2950262427330017, 0.01715547777712345, 0.6426549553871155, 0.5927669405937195, -5.346794420477772e-41, 5.55222477534779e-41, 0.34746941924095154, 0.4268692135810852, 0.3387102782726288, -5.369075066060537e-41, 0.17081382870674133, 0.284726619720459, 4.916595791930053e-41, 0.34334155917167664, 0.14192770421504974, 0.1621793806552887, 0.3173811733722687, 0.256369948387146, 0.47670984268188477, 0.1452210247516632, 0.29336848855018616, 0.23180639743804932, 0.619472324848175, 0.4158211350440979]
[2025-06-12 17:23:01,282]: Mean: 0.22197531
[2025-06-12 17:23:01,282]: Min: -0.00000000
[2025-06-12 17:23:01,282]: Max: 0.64265496
[2025-06-12 17:23:01,284]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-12 17:23:01,356]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 17:23:01,357]: Mean: 0.00000000
[2025-06-12 17:23:01,357]: Min: -0.00063665
[2025-06-12 17:23:01,357]: Max: 0.00084887
[2025-06-12 17:23:01,357]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-06-12 17:23:01,358]: Sample Values (25 elements): [4.953169681848931e-41, 4.926124621487462e-41, -4.913232675615674e-41, 4.930608776573301e-41, -4.925283842408867e-41, 4.926825270719624e-41, 4.936354100277033e-41, -5.60869710346008e-41, 4.91813722024081e-41, -4.907207092219077e-41, -4.920659557476595e-41, -5.045795510340801e-41, -5.959442109080582e-41, 6.284122963264642e-41, -5.041731744794259e-41, 5.796190837986741e-41, 4.908608390683402e-41, 5.935900294879925e-41, 5.056024989130372e-41, 4.907347222065509e-41, 4.909589299608429e-41, 5.49351036969258e-41, 5.984665481438429e-41, -0.004261521622538567, -4.94462176121655e-41]
[2025-06-12 17:23:01,358]: Mean: 0.00020141
[2025-06-12 17:23:01,358]: Min: -0.01097126
[2025-06-12 17:23:01,358]: Max: 0.08110429
[2025-06-12 17:23:01,360]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-12 17:23:01,443]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 17:23:01,444]: Mean: -0.00000000
[2025-06-12 17:23:01,444]: Min: -0.00115138
[2025-06-12 17:23:01,444]: Max: 0.00115138
[2025-06-12 17:23:01,445]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-06-12 17:23:01,447]: Sample Values (25 elements): [3.706844831086187e-10, -0.0018953437684103847, 0.0009209270356222987, 4.917997090394378e-41, -0.0004664628941100091, 6.621334864887351e-33, 8.943341900380375e-11, 5.486083487831659e-41, 5.858408489802763e-41, 5.925530686243921e-41, 8.413622687442057e-09, -0.0009548933594487607, 0.004821238573640585, 0.0017593125812709332, 8.997897538165489e-09, -2.0908952080844756e-07, -1.2324775284599024e-19, 8.597956722489378e-24, 4.765168704068401e-09, -4.906506442986914e-41, -5.152574453322352e-41, 0.002773142885416746, -4.905245274369022e-41, -4.420579458042084e-08, -5.481739462592252e-41]
[2025-06-12 17:23:01,447]: Mean: 0.00085891
[2025-06-12 17:23:01,448]: Min: -0.02137157
[2025-06-12 17:23:01,448]: Max: 0.04153922
[2025-06-12 17:23:01,448]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-06-12 17:23:01,448]: Sample Values (25 elements): [4.905385404215455e-41, 5.628035022267763e-41, -0.14189960062503815, -0.15666720271110535, 0.18724264204502106, -0.21916429698467255, 0.32265424728393555, -0.24136589467525482, -0.1387917697429657, 0.11426091194152832, -0.3778172433376312, -0.10225441306829453, -3.364562317642594e-08, -0.00937300082296133, -0.1331310123205185, 3.233665265575335e-28, 0.04214403033256531, -0.20085754990577698, 0.012431453913450241, 0.0007260541897267103, 0.10575014352798462, -0.3293062448501587, -7.624132194905542e-06, -4.971106302192289e-41, 0.0019653444178402424]
[2025-06-12 17:23:01,449]: Mean: -0.01965920
[2025-06-12 17:23:01,449]: Min: -0.66247123
[2025-06-12 17:23:01,449]: Max: 0.39744562
[2025-06-12 17:23:01,449]: 


QAT of ResNet18 with parametrized_relu down to 2 bits...
[2025-06-12 17:23:01,741]: [ResNet18_parametrized_relu_quantized_2_bits] after configure_qat:
[2025-06-12 17:23:01,780]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-06-12 17:25:24,031]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 001 Train Loss: 0.5812 Train Acc: 0.7988 Eval Loss: 0.5195 Eval Acc: 0.8297 (LR: 0.00100000)
[2025-06-12 17:27:45,721]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 002 Train Loss: 0.4482 Train Acc: 0.8450 Eval Loss: 0.5415 Eval Acc: 0.8279 (LR: 0.00100000)
[2025-06-12 17:30:08,182]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 003 Train Loss: 0.4199 Train Acc: 0.8549 Eval Loss: 0.4827 Eval Acc: 0.8432 (LR: 0.00100000)
[2025-06-12 17:32:30,021]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 004 Train Loss: 0.4119 Train Acc: 0.8575 Eval Loss: 0.5462 Eval Acc: 0.8292 (LR: 0.00100000)
[2025-06-12 17:34:52,017]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 005 Train Loss: 0.4050 Train Acc: 0.8586 Eval Loss: 0.4841 Eval Acc: 0.8419 (LR: 0.00100000)
[2025-06-12 17:37:14,061]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 006 Train Loss: 0.4132 Train Acc: 0.8570 Eval Loss: 0.5587 Eval Acc: 0.8194 (LR: 0.00100000)
[2025-06-12 17:39:36,271]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 007 Train Loss: 0.4081 Train Acc: 0.8596 Eval Loss: 0.5172 Eval Acc: 0.8344 (LR: 0.00100000)
[2025-06-12 17:41:58,843]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 008 Train Loss: 0.4122 Train Acc: 0.8591 Eval Loss: 0.5441 Eval Acc: 0.8264 (LR: 0.00100000)
[2025-06-12 17:44:20,570]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 009 Train Loss: 0.4184 Train Acc: 0.8562 Eval Loss: 0.4470 Eval Acc: 0.8505 (LR: 0.00100000)
[2025-06-12 17:46:42,126]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 010 Train Loss: 0.4173 Train Acc: 0.8564 Eval Loss: 0.5919 Eval Acc: 0.8126 (LR: 0.00100000)
[2025-06-12 17:49:03,631]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 011 Train Loss: 0.4210 Train Acc: 0.8534 Eval Loss: 0.5390 Eval Acc: 0.8222 (LR: 0.00100000)
[2025-06-12 17:51:24,728]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 012 Train Loss: 0.4184 Train Acc: 0.8564 Eval Loss: 0.4905 Eval Acc: 0.8402 (LR: 0.00100000)
[2025-06-12 17:53:46,049]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 013 Train Loss: 0.4133 Train Acc: 0.8576 Eval Loss: 0.5063 Eval Acc: 0.8305 (LR: 0.00100000)
[2025-06-12 17:56:07,332]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 014 Train Loss: 0.4144 Train Acc: 0.8568 Eval Loss: 0.5838 Eval Acc: 0.8146 (LR: 0.00100000)
[2025-06-12 17:58:28,702]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 015 Train Loss: 0.4159 Train Acc: 0.8579 Eval Loss: 0.7587 Eval Acc: 0.7691 (LR: 0.00010000)
[2025-06-12 18:00:50,459]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 016 Train Loss: 0.3217 Train Acc: 0.8892 Eval Loss: 0.3180 Eval Acc: 0.8927 (LR: 0.00010000)
[2025-06-12 18:03:11,979]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 017 Train Loss: 0.2941 Train Acc: 0.8995 Eval Loss: 0.3192 Eval Acc: 0.8941 (LR: 0.00010000)
[2025-06-12 18:05:33,258]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 018 Train Loss: 0.2844 Train Acc: 0.9008 Eval Loss: 0.3227 Eval Acc: 0.8931 (LR: 0.00010000)
[2025-06-12 18:07:54,613]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 019 Train Loss: 0.2822 Train Acc: 0.9027 Eval Loss: 0.3108 Eval Acc: 0.8951 (LR: 0.00010000)
[2025-06-12 18:10:16,467]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 020 Train Loss: 0.2748 Train Acc: 0.9031 Eval Loss: 0.3329 Eval Acc: 0.8914 (LR: 0.00010000)
[2025-06-12 18:12:37,978]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 021 Train Loss: 0.2751 Train Acc: 0.9047 Eval Loss: 0.3148 Eval Acc: 0.8996 (LR: 0.00010000)
[2025-06-12 18:14:59,423]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 022 Train Loss: 0.2679 Train Acc: 0.9061 Eval Loss: 0.3187 Eval Acc: 0.8979 (LR: 0.00010000)
[2025-06-12 18:17:20,869]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 023 Train Loss: 0.2630 Train Acc: 0.9086 Eval Loss: 0.3465 Eval Acc: 0.8876 (LR: 0.00010000)
[2025-06-12 18:19:42,167]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 024 Train Loss: 0.2647 Train Acc: 0.9078 Eval Loss: 0.3249 Eval Acc: 0.8971 (LR: 0.00010000)
[2025-06-12 18:22:05,933]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 025 Train Loss: 0.2664 Train Acc: 0.9074 Eval Loss: 0.3267 Eval Acc: 0.8929 (LR: 0.00001000)
[2025-06-12 18:24:27,302]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 026 Train Loss: 0.2427 Train Acc: 0.9155 Eval Loss: 0.3086 Eval Acc: 0.9003 (LR: 0.00001000)
[2025-06-12 18:26:49,094]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 027 Train Loss: 0.2385 Train Acc: 0.9171 Eval Loss: 0.2943 Eval Acc: 0.9047 (LR: 0.00001000)
[2025-06-12 18:29:10,913]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 028 Train Loss: 0.2328 Train Acc: 0.9193 Eval Loss: 0.2929 Eval Acc: 0.9056 (LR: 0.00001000)
[2025-06-12 18:31:32,572]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 029 Train Loss: 0.2310 Train Acc: 0.9201 Eval Loss: 0.2783 Eval Acc: 0.9096 (LR: 0.00001000)
[2025-06-12 18:33:53,959]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 030 Train Loss: 0.2285 Train Acc: 0.9209 Eval Loss: 0.2846 Eval Acc: 0.9070 (LR: 0.00001000)
[2025-06-12 18:36:16,006]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 031 Train Loss: 0.2297 Train Acc: 0.9211 Eval Loss: 0.2869 Eval Acc: 0.9086 (LR: 0.00001000)
[2025-06-12 18:38:38,206]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 032 Train Loss: 0.2288 Train Acc: 0.9191 Eval Loss: 0.2960 Eval Acc: 0.9036 (LR: 0.00001000)
[2025-06-12 18:40:33,390]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 033 Train Loss: 0.2282 Train Acc: 0.9209 Eval Loss: 0.2931 Eval Acc: 0.9056 (LR: 0.00001000)
[2025-06-12 18:42:20,493]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 034 Train Loss: 0.2283 Train Acc: 0.9199 Eval Loss: 0.2942 Eval Acc: 0.9062 (LR: 0.00001000)
[2025-06-12 18:44:06,790]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 035 Train Loss: 0.2293 Train Acc: 0.9207 Eval Loss: 0.2892 Eval Acc: 0.9073 (LR: 0.00000100)
[2025-06-12 18:45:56,337]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 036 Train Loss: 0.2196 Train Acc: 0.9232 Eval Loss: 0.2796 Eval Acc: 0.9090 (LR: 0.00000100)
[2025-06-12 18:47:43,556]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 037 Train Loss: 0.2211 Train Acc: 0.9239 Eval Loss: 0.2841 Eval Acc: 0.9111 (LR: 0.00000100)
[2025-06-12 18:49:28,119]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 038 Train Loss: 0.2188 Train Acc: 0.9242 Eval Loss: 0.2851 Eval Acc: 0.9097 (LR: 0.00000100)
[2025-06-12 18:51:16,037]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 039 Train Loss: 0.2161 Train Acc: 0.9237 Eval Loss: 0.2836 Eval Acc: 0.9097 (LR: 0.00000100)
[2025-06-12 18:53:03,968]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 040 Train Loss: 0.2167 Train Acc: 0.9241 Eval Loss: 0.2841 Eval Acc: 0.9067 (LR: 0.00000100)
[2025-06-12 18:54:52,352]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 041 Train Loss: 0.2173 Train Acc: 0.9251 Eval Loss: 0.2805 Eval Acc: 0.9084 (LR: 0.00000010)
[2025-06-12 18:56:40,568]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 042 Train Loss: 0.2157 Train Acc: 0.9240 Eval Loss: 0.2836 Eval Acc: 0.9104 (LR: 0.00000010)
[2025-06-12 18:58:30,477]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 043 Train Loss: 0.2158 Train Acc: 0.9247 Eval Loss: 0.2833 Eval Acc: 0.9074 (LR: 0.00000010)
[2025-06-12 19:00:23,269]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 044 Train Loss: 0.2106 Train Acc: 0.9268 Eval Loss: 0.2844 Eval Acc: 0.9053 (LR: 0.00000010)
[2025-06-12 19:02:19,085]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 045 Train Loss: 0.2169 Train Acc: 0.9240 Eval Loss: 0.2787 Eval Acc: 0.9108 (LR: 0.00000010)
[2025-06-12 19:04:09,763]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 046 Train Loss: 0.2130 Train Acc: 0.9252 Eval Loss: 0.2907 Eval Acc: 0.9070 (LR: 0.00000010)
[2025-06-12 19:06:01,459]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 047 Train Loss: 0.2141 Train Acc: 0.9253 Eval Loss: 0.2860 Eval Acc: 0.9080 (LR: 0.00000010)
[2025-06-12 19:07:51,552]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 048 Train Loss: 0.2124 Train Acc: 0.9259 Eval Loss: 0.2816 Eval Acc: 0.9089 (LR: 0.00000010)
[2025-06-12 19:09:36,114]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 049 Train Loss: 0.2161 Train Acc: 0.9251 Eval Loss: 0.2849 Eval Acc: 0.9084 (LR: 0.00000010)
[2025-06-12 19:11:24,855]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 050 Train Loss: 0.2159 Train Acc: 0.9246 Eval Loss: 0.2825 Eval Acc: 0.9084 (LR: 0.00000010)
[2025-06-12 19:13:12,771]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 051 Train Loss: 0.2147 Train Acc: 0.9253 Eval Loss: 0.2775 Eval Acc: 0.9098 (LR: 0.00000010)
[2025-06-12 19:15:03,755]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 052 Train Loss: 0.2199 Train Acc: 0.9249 Eval Loss: 0.2842 Eval Acc: 0.9049 (LR: 0.00000010)
[2025-06-12 19:16:54,668]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 053 Train Loss: 0.2124 Train Acc: 0.9260 Eval Loss: 0.2835 Eval Acc: 0.9072 (LR: 0.00000010)
[2025-06-12 19:18:45,988]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 054 Train Loss: 0.2136 Train Acc: 0.9253 Eval Loss: 0.2821 Eval Acc: 0.9086 (LR: 0.00000010)
[2025-06-12 19:20:37,034]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 055 Train Loss: 0.2144 Train Acc: 0.9259 Eval Loss: 0.2859 Eval Acc: 0.9069 (LR: 0.00000010)
[2025-06-12 19:22:27,909]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 056 Train Loss: 0.2150 Train Acc: 0.9252 Eval Loss: 0.2839 Eval Acc: 0.9077 (LR: 0.00000010)
[2025-06-12 19:24:18,888]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 057 Train Loss: 0.2167 Train Acc: 0.9239 Eval Loss: 0.2846 Eval Acc: 0.9071 (LR: 0.00000010)
[2025-06-12 19:26:09,866]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 058 Train Loss: 0.2158 Train Acc: 0.9249 Eval Loss: 0.2839 Eval Acc: 0.9083 (LR: 0.00000010)
[2025-06-12 19:28:00,555]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 059 Train Loss: 0.2112 Train Acc: 0.9268 Eval Loss: 0.2843 Eval Acc: 0.9066 (LR: 0.00000010)
[2025-06-12 19:29:44,513]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 060 Train Loss: 0.2176 Train Acc: 0.9255 Eval Loss: 0.2818 Eval Acc: 0.9090 (LR: 0.00000010)
[2025-06-12 19:29:44,513]: [ResNet18_parametrized_relu_quantized_2_bits] Best Eval Accuracy: 0.9111
[2025-06-12 19:29:44,605]: 


Quantization of model down to 2 bits finished
[2025-06-12 19:29:44,605]: Model Architecture:
[2025-06-12 19:29:44,651]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7593], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.277907371520996)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2555], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.393311470746994, max_val=0.37308263778686523)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6873], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2638], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3681556284427643, max_val=0.4233841896057129)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1275], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2989], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.44965577125549316, max_val=0.44697484374046326)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6462], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2241], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3251638412475586, max_val=0.3470836877822876)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.2364], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2896], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.43443042039871216, max_val=0.43451428413391113)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8463], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2220], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3352581858634949, max_val=0.33084481954574585)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3372], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4862222671508789, max_val=0.5252772569656372)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0757], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2054], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30825620889663696, max_val=0.3080647587776184)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8289], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1963], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2758234143257141, max_val=0.31295913457870483)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.2496], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1875], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2596040964126587, max_val=0.3028896450996399)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9515], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2188], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2678857147693634, max_val=0.3883877396583557)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2382], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.35662224888801575, max_val=0.35800349712371826)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0809], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2183], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30387061834335327, max_val=0.35097095370292664)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8366], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1803], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2642424702644348, max_val=0.2767704725265503)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1641], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1917], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2425164431333542, max_val=0.33269840478897095)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0278], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1322], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20276552438735962, max_val=0.19396059215068817)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2181], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32664167881011963, max_val=0.3277584910392761)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0084], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0546], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06891772896051407, max_val=0.09485844522714615)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1752], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0717], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10729538649320602, max_val=0.1079077422618866)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0085], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-06-12 19:29:44,651]: 
Model Weights:
[2025-06-12 19:29:44,651]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-06-12 19:29:44,652]: Sample Values (25 elements): [0.19952546060085297, 0.08781208842992783, -0.1489597111940384, 0.0889526978135109, -0.038663335144519806, -0.08186611533164978, 0.15339425206184387, 0.07707028090953827, -0.29604509472846985, -0.035001859068870544, 0.26268085837364197, -0.19302862882614136, -0.16909903287887573, 0.09824822098016739, -0.1437082290649414, -0.18984070420265198, -0.032083626836538315, 0.06335002183914185, 0.10363811254501343, 0.2868470549583435, -0.2880864143371582, 0.08766203373670578, -0.060813672840595245, -0.16518698632717133, -0.11921683698892593]
[2025-06-12 19:29:44,652]: Mean: -0.00101113
[2025-06-12 19:29:44,652]: Min: -0.45989528
[2025-06-12 19:29:44,652]: Max: 0.41883197
[2025-06-12 19:29:44,652]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-06-12 19:29:44,652]: Sample Values (25 elements): [1.4227962493896484, 0.8983973264694214, 0.75672447681427, 0.9929177761077881, 1.102950096130371, 0.9258275628089905, 0.8704097867012024, 0.9934595227241516, 0.2695557177066803, 0.79939866065979, 0.685692548751831, 0.8929889798164368, 1.0324432849884033, 0.6047124266624451, 0.5909437537193298, 0.5200393199920654, 1.008412480354309, 0.6997723579406738, 0.8243055939674377, 0.5975102186203003, 1.0171295404434204, 1.3331276178359985, 0.5472536087036133, 0.7659062743186951, 0.9034252762794495]
[2025-06-12 19:29:44,652]: Mean: 0.81738049
[2025-06-12 19:29:44,653]: Min: 0.26955572
[2025-06-12 19:29:44,653]: Max: 1.62895751
[2025-06-12 19:29:44,654]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 19:29:44,654]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.25546470284461975, 0.0, -0.25546470284461975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,654]: Mean: -0.00458761
[2025-06-12 19:29:44,655]: Min: -0.51092941
[2025-06-12 19:29:44,655]: Max: 0.25546470
[2025-06-12 19:29:44,655]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-06-12 19:29:44,655]: Sample Values (25 elements): [0.45428022742271423, 0.48177844285964966, 0.3562995195388794, 0.36389097571372986, 0.4587428867816925, 0.7345789670944214, 4.7615474613849074e-05, 0.6771384477615356, 0.786822497844696, 0.48875248432159424, 0.8802189230918884, 0.6904825568199158, 0.6700356602668762, 0.5952552556991577, 0.7335836291313171, 0.7001479864120483, 0.8117015361785889, -4.928086439337517e-41, 0.8321019411087036, 0.7342010140419006, 0.7047401666641235, 0.7918326258659363, 0.4512632489204407, 0.5728739500045776, 0.6935051679611206]
[2025-06-12 19:29:44,655]: Mean: 0.62843591
[2025-06-12 19:29:44,655]: Min: -0.00000000
[2025-06-12 19:29:44,656]: Max: 1.05396259
[2025-06-12 19:29:44,656]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 19:29:44,657]: Sample Values (25 elements): [0.26384660601615906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26384660601615906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,657]: Mean: 0.00074436
[2025-06-12 19:29:44,657]: Min: -0.26384661
[2025-06-12 19:29:44,657]: Max: 0.52769321
[2025-06-12 19:29:44,657]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-06-12 19:29:44,658]: Sample Values (25 elements): [0.443602979183197, 0.8359516263008118, 0.8493083119392395, 1.006353735923767, 0.7598623037338257, 0.6628502607345581, 0.47223392128944397, 0.7007368803024292, 0.7417178750038147, 0.76109778881073, 0.7105545401573181, 0.7913573980331421, 0.8500633239746094, 0.60377037525177, 0.7222453355789185, 1.1387938261032104, 0.5813265442848206, 0.6207810640335083, 0.668627917766571, 0.4193483591079712, 0.548103928565979, 1.2559313774108887, 0.6644641160964966, 0.8815389275550842, 0.46520760655403137]
[2025-06-12 19:29:44,658]: Mean: 0.74553370
[2025-06-12 19:29:44,658]: Min: 0.31124094
[2025-06-12 19:29:44,658]: Max: 1.25593138
[2025-06-12 19:29:44,660]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 19:29:44,660]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,660]: Mean: -0.00042970
[2025-06-12 19:29:44,660]: Min: -0.59775370
[2025-06-12 19:29:44,661]: Max: 0.29887685
[2025-06-12 19:29:44,661]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-06-12 19:29:44,661]: Sample Values (25 elements): [1.1393492221832275, 0.42807942628860474, 0.5901407599449158, 0.3149773180484772, 0.8687345385551453, 0.6642972230911255, 0.500698447227478, 0.5879577398300171, 0.8010172843933105, 0.7431932091712952, 8.865820404213576e-12, 0.5002950429916382, 1.0560098886489868, 0.8351914882659912, 0.6340349912643433, 0.0004110948066227138, 0.5977158546447754, 0.7150698304176331, -4.916595791930053e-41, 0.277781218290329, 0.7012921571731567, 0.6214011311531067, 0.5668631196022034, 0.8876147270202637, 0.6915676593780518]
[2025-06-12 19:29:44,661]: Mean: 0.57348680
[2025-06-12 19:29:44,661]: Min: -0.00000000
[2025-06-12 19:29:44,661]: Max: 1.13934922
[2025-06-12 19:29:44,662]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 19:29:44,663]: Sample Values (25 elements): [0.0, 0.22408251464366913, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.22408251464366913, 0.0, 0.0, 0.0, -0.22408251464366913, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,663]: Mean: -0.00025530
[2025-06-12 19:29:44,663]: Min: -0.22408251
[2025-06-12 19:29:44,663]: Max: 0.44816503
[2025-06-12 19:29:44,663]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-06-12 19:29:44,663]: Sample Values (25 elements): [0.5447891354560852, 0.9760358929634094, 0.5378231406211853, 0.8806470632553101, 0.7341201901435852, 0.7686554789543152, 0.7457553744316101, 0.9437447190284729, 1.139971375465393, 0.5061514377593994, 0.6868466138839722, 0.4959125220775604, 0.4915980398654938, 1.0260682106018066, 0.7680774927139282, 0.8661825656890869, 0.8338872194290161, 0.5596470236778259, 0.9410879015922546, 0.8114800453186035, 0.8863922357559204, 0.7359464764595032, 0.8765797019004822, 0.845255434513092, 0.5767521858215332]
[2025-06-12 19:29:44,664]: Mean: 0.73863471
[2025-06-12 19:29:44,664]: Min: 0.38720751
[2025-06-12 19:29:44,664]: Max: 1.14609861
[2025-06-12 19:29:44,665]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-06-12 19:29:44,666]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,666]: Mean: 0.00117858
[2025-06-12 19:29:44,666]: Min: -0.28964823
[2025-06-12 19:29:44,666]: Max: 0.57929647
[2025-06-12 19:29:44,666]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-06-12 19:29:44,667]: Sample Values (25 elements): [0.6687654256820679, 4.912672156229944e-41, 0.5967161059379578, 1.0300688743591309, 0.6605660319328308, 0.5319982171058655, 0.723094642162323, 0.7408286929130554, 0.7285511493682861, -4.953449941541796e-41, 0.7939682006835938, 0.8813058733940125, 0.4337327778339386, 0.7566417455673218, 0.5162346959114075, 8.704634092282504e-05, 0.6806397438049316, 0.6093180179595947, -4.969985263420829e-41, 0.7939419746398926, 0.7214329242706299, 5.190409511859122e-41, 0.5387384295463562, 0.7035557627677917, 0.7513469457626343]
[2025-06-12 19:29:44,667]: Mean: 0.51911777
[2025-06-12 19:29:44,667]: Min: -0.00000000
[2025-06-12 19:29:44,667]: Max: 1.03006887
[2025-06-12 19:29:44,668]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-12 19:29:44,670]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,670]: Mean: 0.00018069
[2025-06-12 19:29:44,670]: Min: -0.44406867
[2025-06-12 19:29:44,670]: Max: 0.22203434
[2025-06-12 19:29:44,670]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-06-12 19:29:44,670]: Sample Values (25 elements): [0.829789936542511, 0.000759007059969008, 0.6666806936264038, 0.5767792463302612, 0.5130977630615234, 0.6572347283363342, 0.6391173601150513, 0.7504353523254395, 0.7240260243415833, 0.7449290752410889, 0.7976343631744385, 0.6261743903160095, 0.8281829953193665, 0.684651792049408, 0.8570183515548706, 0.6325984001159668, 0.7454528212547302, 0.8279465436935425, 0.5818853974342346, 0.7083436250686646, 0.705014705657959, 0.5318179726600647, 0.710353672504425, 0.4892517924308777, 0.5530239939689636]
[2025-06-12 19:29:44,670]: Mean: 0.65976965
[2025-06-12 19:29:44,671]: Min: -0.00000000
[2025-06-12 19:29:44,671]: Max: 1.01375186
[2025-06-12 19:29:44,672]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-06-12 19:29:44,672]: Sample Values (25 elements): [0.0, 0.33716651797294617, 0.0, 0.33716651797294617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.33716651797294617, -0.33716651797294617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,672]: Mean: -0.00164632
[2025-06-12 19:29:44,672]: Min: -0.33716652
[2025-06-12 19:29:44,673]: Max: 0.67433304
[2025-06-12 19:29:44,673]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-06-12 19:29:44,673]: Sample Values (25 elements): [0.5668855905532837, 0.49503281712532043, 0.55152428150177, 0.7219652533531189, 0.33340418338775635, 0.5120518207550049, 0.5114233493804932, 0.4940278232097626, 0.5267569422721863, 0.281902939081192, 0.4587104618549347, 0.4300381541252136, 0.7376944422721863, 0.44038835167884827, 0.5274523496627808, 0.5074276924133301, 7.428388198604807e-05, 0.4849058985710144, 0.41003185510635376, 0.3311593234539032, 0.49203523993492126, 0.3120059669017792, 0.5199968814849854, 0.45076096057891846, 0.5349860787391663]
[2025-06-12 19:29:44,673]: Mean: 0.43045512
[2025-06-12 19:29:44,673]: Min: -0.00000000
[2025-06-12 19:29:44,673]: Max: 0.85263371
[2025-06-12 19:29:44,674]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-12 19:29:44,676]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,676]: Mean: -0.00011285
[2025-06-12 19:29:44,676]: Min: -0.41088066
[2025-06-12 19:29:44,677]: Max: 0.20544033
[2025-06-12 19:29:44,677]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-06-12 19:29:44,677]: Sample Values (25 elements): [0.5818860530853271, 0.6650402545928955, 0.680156409740448, 0.6937315464019775, 0.6592721343040466, 0.48234036564826965, -5.159160556104679e-41, 0.6385772228240967, 0.8182279467582703, 0.6799877285957336, 0.24656963348388672, 0.508143424987793, 0.26896756887435913, 3.519010194352745e-09, -4.965080718795692e-41, 0.6383947730064392, 0.3457682728767395, 4.965781368027854e-41, 0.5158435702323914, 0.7150099873542786, 0.4176059067249298, -6.191216875079907e-41, 0.2911767065525055, 0.6518905758857727, 0.5520467758178711]
[2025-06-12 19:29:44,677]: Mean: 0.45941061
[2025-06-12 19:29:44,677]: Min: -0.00000000
[2025-06-12 19:29:44,677]: Max: 0.81822795
[2025-06-12 19:29:44,678]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-12 19:29:44,680]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,680]: Mean: -0.00036868
[2025-06-12 19:29:44,680]: Min: -0.19626085
[2025-06-12 19:29:44,680]: Max: 0.39252171
[2025-06-12 19:29:44,680]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-06-12 19:29:44,681]: Sample Values (25 elements): [0.680871307849884, 0.8276113271713257, 0.4191865026950836, 0.5667757391929626, 0.39904317259788513, 0.5700432062149048, 0.520804226398468, 0.27303197979927063, 0.5562021732330322, 0.5003231167793274, 0.6397054195404053, 0.08124379813671112, 0.5325099229812622, 0.38082441687583923, 0.26435425877571106, 0.6492236256599426, 0.40330639481544495, 0.5356950759887695, 0.5916357636451721, 0.6252521276473999, 0.25963887572288513, 0.5267068147659302, 0.635373055934906, 0.5751919150352478, 0.7507635354995728]
[2025-06-12 19:29:44,681]: Mean: 0.53662586
[2025-06-12 19:29:44,681]: Min: -0.00000000
[2025-06-12 19:29:44,681]: Max: 0.85513055
[2025-06-12 19:29:44,682]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-06-12 19:29:44,685]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,685]: Mean: 0.00047365
[2025-06-12 19:29:44,685]: Min: -0.18749791
[2025-06-12 19:29:44,686]: Max: 0.37499583
[2025-06-12 19:29:44,686]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-06-12 19:29:44,686]: Sample Values (25 elements): [-5.462261413938137e-41, 0.04900294169783592, 0.6491187214851379, 0.6940253973007202, 4.919818778398e-41, -6.291409715279131e-41, 0.6779997944831848, 0.7025659680366516, -5.032062785390418e-41, 0.6350748538970947, 0.4153696596622467, -5.721781889531093e-41, 5.063311741144862e-41, 5.694721089003277e-15, 0.5008461475372314, 0.6180757284164429, 0.6090108156204224, 0.7230548858642578, -6.078972868087489e-41, 0.5922749638557434, 0.35509902238845825, 0.5196391344070435, 0.5731221437454224, -5.016788632129278e-41, 0.5715765357017517]
[2025-06-12 19:29:44,686]: Mean: 0.34491622
[2025-06-12 19:29:44,686]: Min: -0.00000000
[2025-06-12 19:29:44,686]: Max: 0.86296630
[2025-06-12 19:29:44,687]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-12 19:29:44,695]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,695]: Mean: 0.00036644
[2025-06-12 19:29:44,695]: Min: -0.21875784
[2025-06-12 19:29:44,695]: Max: 0.43751568
[2025-06-12 19:29:44,695]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-06-12 19:29:44,695]: Sample Values (25 elements): [4.95877487570623e-41, 4.949946695380984e-41, 0.5122103691101074, 0.5533183217048645, 0.5701350569725037, 0.4760623872280121, 4.985679806221267e-41, 0.5686304569244385, 0.6648863554000854, 0.7675886750221252, 0.6130618453025818, 0.5964964032173157, 0.5464772582054138, 0.47052013874053955, 0.6335449814796448, 0.35457420349121094, 0.5597935318946838, 0.6311303973197937, 0.5273779630661011, 0.22989587485790253, 0.5259228348731995, 6.225268427763e-41, 0.634413480758667, 0.9718618392944336, 0.53449547290802]
[2025-06-12 19:29:44,696]: Mean: 0.41706446
[2025-06-12 19:29:44,696]: Min: -0.00000000
[2025-06-12 19:29:44,696]: Max: 1.01297057
[2025-06-12 19:29:44,697]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-06-12 19:29:44,697]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.2382085919380188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2382085919380188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,697]: Mean: 0.00018901
[2025-06-12 19:29:44,698]: Min: -0.23820859
[2025-06-12 19:29:44,698]: Max: 0.47641718
[2025-06-12 19:29:44,698]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-06-12 19:29:44,698]: Sample Values (25 elements): [9.856936412723157e-22, 0.3668253421783447, 0.4857032001018524, 0.43165087699890137, 0.1345003992319107, 0.10999143868684769, 0.06318018585443497, 0.10649367421865463, 0.0012862617149949074, 0.0812569335103035, 0.16450993716716766, 0.2612454891204834, 4.978953573592508e-41, 0.20272229611873627, 0.3083564043045044, 0.2966326177120209, -5.011323568118411e-41, 0.31001347303390503, 0.14081019163131714, 0.02227354422211647, 4.90566566390832e-41, 0.33958691358566284, 0.3902747929096222, 0.25525718927383423, 0.24386920034885406]
[2025-06-12 19:29:44,698]: Mean: 0.21181543
[2025-06-12 19:29:44,698]: Min: -0.00000000
[2025-06-12 19:29:44,699]: Max: 0.58339161
[2025-06-12 19:29:44,699]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-12 19:29:44,706]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21828052401542664, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,706]: Mean: 0.00010732
[2025-06-12 19:29:44,706]: Min: -0.21828052
[2025-06-12 19:29:44,706]: Max: 0.43656105
[2025-06-12 19:29:44,706]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-06-12 19:29:44,707]: Sample Values (25 elements): [0.492816299200058, 4.954711110159688e-41, 4.975870716970993e-41, -6.171878956272224e-41, 5.611639830235162e-41, 0.6411789655685425, 0.30577895045280457, -6.033570797843365e-41, 5.815388626947991e-41, 0.39972275495529175, 5.025476682608091e-41, 0.495309978723526, 0.6439847946166992, 0.3955892026424408, 5.567218668916066e-41, 0.4899108111858368, -5.622710088103328e-41, 0.5976079106330872, 0.5868868827819824, 5.525600104525619e-41, 4.920799687323028e-41, 5.695297348555354e-41, -4.960736693556285e-41, 0.6595417261123657, 4.922200985787352e-41]
[2025-06-12 19:29:44,707]: Mean: 0.17756996
[2025-06-12 19:29:44,707]: Min: -0.00000000
[2025-06-12 19:29:44,707]: Max: 0.76623815
[2025-06-12 19:29:44,708]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-12 19:29:44,714]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,714]: Mean: 0.00000122
[2025-06-12 19:29:44,714]: Min: -0.18033765
[2025-06-12 19:29:44,715]: Max: 0.36067531
[2025-06-12 19:29:44,715]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-06-12 19:29:44,715]: Sample Values (25 elements): [-5.74196058741737e-41, 0.7168301939964294, -4.991425129924998e-41, 0.43722835183143616, 4.933411373501951e-41, 4.943220462752225e-41, 5.456375960387973e-41, 0.2801790237426758, -5.311201439483922e-41, 0.7032718062400818, -4.911270857765619e-41, -5.120624848335747e-41, 0.663820743560791, 0.2396860122680664, -4.912391896537079e-41, 0.700802743434906, 0.49662911891937256, -4.992826428389323e-41, -5.084331218109734e-41, 4.9802147422104e-41, 0.6412230134010315, 4.987921883764186e-41, 3.0394179617587724e-08, 4.911410987612051e-41, 0.350154846906662]
[2025-06-12 19:29:44,715]: Mean: 0.21963555
[2025-06-12 19:29:44,715]: Min: -0.00000000
[2025-06-12 19:29:44,715]: Max: 0.77775639
[2025-06-12 19:29:44,716]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-06-12 19:29:44,730]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,730]: Mean: 0.00006339
[2025-06-12 19:29:44,730]: Min: -0.19173829
[2025-06-12 19:29:44,730]: Max: 0.38347659
[2025-06-12 19:29:44,730]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-06-12 19:29:44,731]: Sample Values (25 elements): [-5.47655465827425e-41, 5.872141214753146e-41, 4.90566566390832e-41, -6.144413506371458e-41, 5.984525351591996e-41, -5.483701280442307e-41, 5.166167048426303e-41, -5.688010596540865e-41, -5.770967465628894e-41, 5.306296894858785e-41, 6.141751039289241e-41, 4.953169681848931e-41, -4.904824884829725e-41, -5.801375642304743e-41, -4.904824884829725e-41, 4.909589299608429e-41, -5.617245024092462e-41, 5.494491278617608e-41, 4.943360592598657e-41, -4.909449169761997e-41, 0.6885875463485718, 4.989323182228511e-41, 4.942659943366495e-41, 5.769285907471704e-41, -4.919398388858703e-41]
[2025-06-12 19:29:44,731]: Mean: 0.05761869
[2025-06-12 19:29:44,731]: Min: -0.00000000
[2025-06-12 19:29:44,731]: Max: 0.92682749
[2025-06-12 19:29:44,732]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-12 19:29:44,773]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,774]: Mean: 0.00010364
[2025-06-12 19:29:44,774]: Min: -0.26448411
[2025-06-12 19:29:44,774]: Max: 0.13224205
[2025-06-12 19:29:44,774]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-06-12 19:29:44,774]: Sample Values (25 elements): [0.4384693503379822, -5.958040810616257e-41, -4.97573058712456e-41, 0.045387882739305496, 0.6159663796424866, 0.2983512282371521, 0.6288017630577087, 4.968023445570774e-41, -5.210868469438265e-41, -5.05196122358383e-41, 0.2870735228061676, 0.6310051083564758, 4.938035658434223e-41, 0.24162861704826355, -4.957373577241905e-41, 0.12930504977703094, 0.48675423860549927, 0.4975709319114685, 0.5740347504615784, -4.90622618329405e-41, 0.5141171216964722, 0.6409271955490112, -4.952469032616768e-41, 4.909168910069132e-41, 0.27081504464149475]
[2025-06-12 19:29:44,774]: Mean: 0.25941908
[2025-06-12 19:29:44,775]: Min: -0.00000000
[2025-06-12 19:29:44,775]: Max: 0.78785563
[2025-06-12 19:29:44,776]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-06-12 19:29:44,777]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,777]: Mean: 0.00159266
[2025-06-12 19:29:44,777]: Min: -0.21813339
[2025-06-12 19:29:44,778]: Max: 0.43626678
[2025-06-12 19:29:44,778]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-06-12 19:29:44,778]: Sample Values (25 elements): [4.999692790864515e-41, 0.28915783762931824, 0.28989243507385254, 0.3580028712749481, 0.4428793787956238, 4.932430464576924e-41, 0.2901933193206787, 0.31494027376174927, 0.3990635573863983, 5.435076223730235e-41, -4.975029937892398e-41, 0.4934416115283966, 0.34298378229141235, 0.265080064535141, 0.43785130977630615, 0.5209290981292725, 0.35284945368766785, -4.951488123691741e-41, 0.3054424226284027, 0.4378708302974701, -5.166026918579871e-41, 0.3020903468132019, 0.44281309843063354, 0.280965656042099, 0.2515840530395508]
[2025-06-12 19:29:44,778]: Mean: 0.23346777
[2025-06-12 19:29:44,778]: Min: -0.00000000
[2025-06-12 19:29:44,778]: Max: 0.73033404
[2025-06-12 19:29:44,779]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-12 19:29:44,818]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.054592058062553406, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,818]: Mean: -0.00000167
[2025-06-12 19:29:44,819]: Min: -0.05459206
[2025-06-12 19:29:44,819]: Max: 0.10918412
[2025-06-12 19:29:44,819]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-06-12 19:29:44,820]: Sample Values (25 elements): [-5.103108617531686e-41, 4.920519427630163e-41, -4.986660715146294e-41, 4.908047871297672e-41, -4.931869945191194e-41, -4.949666435688119e-41, 4.910289948840591e-41, 4.94013760613071e-41, 5.797592136451066e-41, 4.942519813520062e-41, -4.94013760613071e-41, -5.274487419718611e-41, 5.425127004633529e-41, 4.92430293348384e-41, 6.162490256561248e-41, -5.031362136158256e-41, 4.908047871297672e-41, 4.909028780222699e-41, 4.938315918127088e-41, -4.913793195001404e-41, 5.190129252166257e-41, -4.911130727919186e-41, -4.922341115633785e-41, 5.726966693849095e-41, -4.916175402390756e-41]
[2025-06-12 19:29:44,821]: Mean: 0.00051759
[2025-06-12 19:29:44,821]: Min: -0.09576076
[2025-06-12 19:29:44,821]: Max: 0.19642173
[2025-06-12 19:29:44,822]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-12 19:29:44,866]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-12 19:29:44,867]: Mean: -0.00000240
[2025-06-12 19:29:44,867]: Min: -0.07173438
[2025-06-12 19:29:44,867]: Max: 0.14346877
[2025-06-12 19:29:44,867]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-06-12 19:29:44,868]: Sample Values (25 elements): [2.2374844199646304e-09, -5.994194310995838e-41, 0.00010097588528878987, -5.472911282267006e-41, -4.936914619662763e-41, -5.589219054805965e-41, -4.904824884829725e-41, 6.139649091592754e-41, -4.910990598072754e-41, 4.996189544703703e-41, -6.216580377284186e-41, 5.028279279536741e-41, 4.906506442986914e-41, -3.24333414027933e-05, 5.264398070775473e-41, -4.930889036266166e-41, 0.057534217834472656, 0.00013889846741221845, 6.077711699469597e-41, 0.013962696306407452, 5.774891101329004e-41, -4.92374241409811e-41, 5.967009120787936e-41, -6.877531660834575e-17, 5.939403541040737e-41]
[2025-06-12 19:29:44,868]: Mean: 0.01037446
[2025-06-12 19:29:44,868]: Min: -0.00229233
[2025-06-12 19:29:44,868]: Max: 0.17706543
[2025-06-12 19:29:44,868]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-06-12 19:29:44,868]: Sample Values (25 elements): [-0.062458813190460205, 0.005268743261694908, 0.027540531009435654, 0.018956564366817474, 0.04033517464995384, -0.21518146991729736, 0.0003132568672299385, 0.06482890993356705, 0.2196638137102127, -0.12383522093296051, -0.11341919004917145, 0.14071087539196014, 0.08706830441951752, -0.06752645969390869, -5.865975501510117e-41, 5.385610387939569e-41, 0.11673571914434433, -0.10936567932367325, -3.2140852333178016e-19, -0.07980438321828842, 0.008193000219762325, -5.818611613415938e-41, -0.08033803105354309, 0.2687881886959076, 0.14172056317329407]
[2025-06-12 19:29:44,869]: Mean: -0.02155510
[2025-06-12 19:29:44,869]: Min: -0.68780226
[2025-06-12 19:29:44,869]: Max: 0.36334127
